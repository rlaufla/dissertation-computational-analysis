{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e58535",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kiwipiepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "\n",
    "kiwi = Kiwi()\n",
    "print(kiwi.analyze(\"Ï†ïÎßê Í∏∞Î∂ÑÏù¥ Ï¢ãÎã§\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd859f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Í∏∞Ï°¥ÏΩîÎìú_successful\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from kiwipiepy import Kiwi\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# ÌïúÍ∏Ä Ìè∞Ìä∏ ÏÑ§Ï†ï\n",
    "matplotlib.rc('font', family='Malgun Gothic')\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ========== ÏÇ¨Ïö©Ïûê ÏÑ§Ï†ï ==========\n",
    "base_dir = r\"C:\\Users\\ÎÖºÎ¨∏\"\n",
    "input_excel_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "senti_path = os.path.join(base_dir, \"for the analysing\", \"data\", \"SentiWord_info.json\")\n",
    "output_excel_path = os.path.join(base_dir, \"3\", \"result.xlsx\")\n",
    "# ================================\n",
    "\n",
    "# 1. Í∞êÏÑ± ÏÇ¨Ï†Ñ Î°úÎìú\n",
    "with open(senti_path, encoding='utf-8-sig') as f:\n",
    "    senti_dict = json.load(f)\n",
    "senti_lookup = {item['word']: int(item['polarity']) for item in senti_dict}\n",
    "\n",
    "# 2. Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
    "df = pd.read_excel(input_excel_path, sheet_name=sheet_name)\n",
    "\n",
    "# 3. periodisation ÏÉùÏÑ±\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979:\n",
    "        return \"1. 1970‚Äì1979\"\n",
    "    elif 1980 <= year <= 1987:\n",
    "        return \"2. 1980‚Äì1987\"\n",
    "    elif 1988 <= year <= 1995:\n",
    "        return \"3. 1988‚Äì1995\"\n",
    "    elif 1996 <= year <= 2007:\n",
    "        return \"4. 1996‚Äì2007\"\n",
    "    elif 2008 <= year <= 2014:\n",
    "        return \"5. 2008‚Äì2014\"\n",
    "    elif 2015 <= year <= 2023:\n",
    "        return \"6. 2015‚Äì2023\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# 4. KIWI Ï¥àÍ∏∞Ìôî\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# 5. Í∏∞ÏÇ¨Î≥Ñ Í∞êÏÑ± Î∂ÑÏÑù\n",
    "records = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = str(row[\"Î≥∏Î¨∏ÎÇ¥Ïö©\"])\n",
    "    period = row[\"Period\"]\n",
    "\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    total_tokens = 0\n",
    "    senti_scores = []\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        word = token.form\n",
    "        tag = token.tag\n",
    "        total_tokens += 1\n",
    "        if tag.startswith(\"VV\") or tag.startswith(\"VA\"):\n",
    "            word += \"Îã§\"\n",
    "        if word in senti_lookup:\n",
    "            score = senti_lookup[word]\n",
    "            senti_scores.append(score)\n",
    "            if score > 0:\n",
    "                pos_count += 1\n",
    "            elif score < 0:\n",
    "                neg_count += 1\n",
    "\n",
    "    senti_count = len(senti_scores)\n",
    "    avg_score = round(np.mean(senti_scores), 3) if senti_scores else 0.0\n",
    "    ratio = round((senti_count / total_tokens * 100), 2) if total_tokens else 0.0\n",
    "\n",
    "    records.append({\n",
    "        \"Î∂ÑÎ•ò\": period,\n",
    "        \"Ï†ÑÏ≤¥Îã®Ïñ¥Ïàò\": total_tokens,\n",
    "        \"Í∞êÏÑ±Îã®Ïñ¥Ïàò\": senti_count,\n",
    "        \"Í∏çÏ†ïÎã®Ïñ¥Ïàò\": pos_count,\n",
    "        \"Î∂ÄÏ†ïÎã®Ïñ¥Ïàò\": neg_count,\n",
    "        \"ÌèâÍ∑†Í∞êÏÑ±Ï†êÏàò\": avg_score,\n",
    "        \"Í∞êÏÑ±Îã®Ïñ¥ÎπÑÏú®\": ratio\n",
    "    })\n",
    "\n",
    "# 6. DataFrameÏúºÎ°ú Ï†ÄÏû•\n",
    "result_df = pd.DataFrame(records)\n",
    "\n",
    "# 7. ÏãúÍ∏∞Î≥Ñ ÌÜµÍ≥Ñ ÏöîÏïΩ\n",
    "summary = result_df.groupby(\"Î∂ÑÎ•ò\").agg({\n",
    "    \"Ï†ÑÏ≤¥Îã®Ïñ¥Ïàò\": \"sum\",\n",
    "    \"Í∞êÏÑ±Îã®Ïñ¥Ïàò\": \"sum\",\n",
    "    \"Í∏çÏ†ïÎã®Ïñ¥Ïàò\": \"sum\",\n",
    "    \"Î∂ÄÏ†ïÎã®Ïñ¥Ïàò\": \"sum\",\n",
    "    \"ÌèâÍ∑†Í∞êÏÑ±Ï†êÏàò\": \"mean\",\n",
    "    \"Í∞êÏÑ±Îã®Ïñ¥ÎπÑÏú®\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "# 8. Ï†ÄÏû•\n",
    "summary.to_excel(output_excel_path, index=False)\n",
    "print(f\"‚úÖ Î∂ÑÏÑù ÏôÑÎ£å! Í≤∞Í≥º Ï†ÄÏû• ÏúÑÏπò: {output_excel_path}\")\n",
    "\n",
    "#Î∞ëÏóê ÏΩîÎìú ÎßêÍ≥† Ïù¥Í±∏Î°ú Ìï†Í±∞ÏûÑ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8493b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ÏàòÏ†ïÏΩîÎìú_ÌòïÌÉúÏÜåÎ∂ÑÏÑù Îã®Ïñ¥ Îçî ÎßéÏù¥ Í∞êÏÑ±Î∂ÑÏÑùÌï†Ïàò ÏûàÍ≤å\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from kiwipiepy import Kiwi\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# ÌïúÍ∏Ä Ìè∞Ìä∏ ÏÑ§Ï†ï\n",
    "matplotlib.rc('font', family='Malgun Gothic')\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ========== ÏÇ¨Ïö©Ïûê ÏÑ§Ï†ï ==========\n",
    "base_dir = r\"C:\\Users\\ÎÖºÎ¨∏\"\n",
    "input_excel_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "senti_path = os.path.join(base_dir, \"for the analysing\", \"data\", \"SentiWord_info.json\")\n",
    "output_excel_path = os.path.join(base_dir, \"3\", \"result.xlsx\")\n",
    "# ================================\n",
    "\n",
    "# 1. Í∞êÏÑ± ÏÇ¨Ï†Ñ Î°úÎìú\n",
    "with open(senti_path, encoding='utf-8-sig') as f:\n",
    "    senti_dict = json.load(f)\n",
    "senti_lookup = {item['word']: int(item['polarity']) for item in senti_dict}\n",
    "\n",
    "# 2. Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
    "df = pd.read_excel(input_excel_path, sheet_name=sheet_name)\n",
    "\n",
    "# 3. periodisation ÏÉùÏÑ±\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979:\n",
    "        return \"1. 1970‚Äì1979\"\n",
    "    elif 1980 <= year <= 1987:\n",
    "        return \"2. 1980‚Äì1987\"\n",
    "    elif 1988 <= year <= 1995:\n",
    "        return \"3. 1988‚Äì1995\"\n",
    "    elif 1996 <= year <= 2007:\n",
    "        return \"4. 1996‚Äì2007\"\n",
    "    elif 2008 <= year <= 2014:\n",
    "        return \"5. 2008‚Äì2014\"\n",
    "    elif 2015 <= year <= 2023:\n",
    "        return \"6. 2015‚Äì2023\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# 4. KIWI Ï¥àÍ∏∞Ìôî\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# 5. Í∞êÏÑ±Ïñ¥ ÌíàÏÇ¨ Ï†ïÏùò (ÎèôÏÇ¨, ÌòïÏö©ÏÇ¨, Î™ÖÏÇ¨, Ïñ¥Í∑º, Î∂ÄÏÇ¨)\n",
    "senti_pos_tags = [\"VV\", \"VA\", \"NNG\", \"XR\", \"MAG\"]\n",
    "\n",
    "# 6. Í∏∞ÏÇ¨Î≥Ñ Í∞êÏÑ± Î∂ÑÏÑù\n",
    "records = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = str(row[\"Î≥∏Î¨∏ÎÇ¥Ïö©\"])\n",
    "    period = row[\"Period\"]\n",
    "\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    total_tokens = 0\n",
    "    senti_scores = []\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        word = token.form\n",
    "        tag = token.tag\n",
    "        total_tokens += 1\n",
    "\n",
    "        # ÎèôÏÇ¨/ÌòïÏö©ÏÇ¨Îäî Ïñ¥Í∞Ñ + Îã§ ÌòïÌÉúÎ°ú Î≥µÏõê\n",
    "        if tag.startswith(\"VV\") or tag.startswith(\"VA\"):\n",
    "            word += \"Îã§\"\n",
    "\n",
    "        # Í∞êÏÑ±ÏÇ¨Ï†ÑÏóê Ìè¨Ìï®Îê† ÌíàÏÇ¨Îßå Îß§Ïπ≠ ÏãúÎèÑ\n",
    "        if tag[:3] in senti_pos_tags or tag in senti_pos_tags:\n",
    "            if word in senti_lookup:\n",
    "                score = senti_lookup[word]\n",
    "                senti_scores.append(score)\n",
    "                if score > 0:\n",
    "                    pos_count += 1\n",
    "                elif score < 0:\n",
    "                    neg_count += 1\n",
    "\n",
    "    senti_count = len(senti_scores)\n",
    "    avg_score = round(np.mean(senti_scores), 3) if senti_scores else 0.0\n",
    "    ratio = round((senti_count / total_tokens * 100), 2) if total_tokens else 0.0\n",
    "\n",
    "    records.append({\n",
    "        \"Î∂ÑÎ•ò\": period,\n",
    "        \"Ï†ÑÏ≤¥Îã®Ïñ¥Ïàò\": total_tokens,\n",
    "        \"Í∞êÏÑ±Îã®Ïñ¥Ïàò\": senti_count,\n",
    "        \"Í∏çÏ†ïÎã®Ïñ¥Ïàò\": pos_count,\n",
    "        \"Î∂ÄÏ†ïÎã®Ïñ¥Ïàò\": neg_count,\n",
    "        \"ÌèâÍ∑†Í∞êÏÑ±Ï†êÏàò\": avg_score,\n",
    "        \"Í∞êÏÑ±Îã®Ïñ¥ÎπÑÏú®\": ratio\n",
    "    })\n",
    "\n",
    "# 7. DataFrameÏúºÎ°ú Ï†ÄÏû•\n",
    "result_df = pd.DataFrame(records)\n",
    "\n",
    "# 8. ÏãúÍ∏∞Î≥Ñ ÌÜµÍ≥Ñ ÏöîÏïΩ\n",
    "summary = result_df.groupby(\"Î∂ÑÎ•ò\").agg({\n",
    "    \"Ï†ÑÏ≤¥Îã®Ïñ¥Ïàò\": \"sum\",\n",
    "    \"Í∞êÏÑ±Îã®Ïñ¥Ïàò\": \"sum\",\n",
    "    \"Í∏çÏ†ïÎã®Ïñ¥Ïàò\": \"sum\",\n",
    "    \"Î∂ÄÏ†ïÎã®Ïñ¥Ïàò\": \"sum\",\n",
    "    \"ÌèâÍ∑†Í∞êÏÑ±Ï†êÏàò\": \"mean\",\n",
    "    \"Í∞êÏÑ±Îã®Ïñ¥ÎπÑÏú®\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "# 9. Ï†ÄÏû•\n",
    "summary.to_excel(output_excel_path, index=False)\n",
    "print(f\"‚úÖ Î∂ÑÏÑù ÏôÑÎ£å! Í≤∞Í≥º Ï†ÄÏû• ÏúÑÏπò: {output_excel_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e8e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ Í∞êÏÑ±Î∂ÑÏÑùÍ≤∞Í≥º\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# ÏÇ¨Ïö©Ïûê ÏÑ§Ï†ï\n",
    "base_dir = r\"C:\\Users\\ÎÖºÎ¨∏\"\n",
    "input_excel_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "senti_path = os.path.join(base_dir, \"for the analysing\", \"data\", \"SentiWord_info.json\")\n",
    "\n",
    "# Í∞êÏÑ±ÏÇ¨Ï†Ñ Î∂àÎü¨Ïò§Í∏∞\n",
    "with open(senti_path, encoding='utf-8-sig') as f:\n",
    "    senti_dict = json.load(f)\n",
    "senti_lookup = {item['word']: int(item['polarity']) for item in senti_dict}\n",
    "\n",
    "# ÌÖçÏä§Ìä∏ Î∂àÎü¨Ïò§Í∏∞\n",
    "df = pd.read_excel(input_excel_path, sheet_name=sheet_name)\n",
    "all_text = \" \".join(df[\"Î≥∏Î¨∏ÎÇ¥Ïö©\"].astype(str).tolist())  # Ï†ÑÏ≤¥ Í∏∞ÏÇ¨ Ìï©ÏπòÍ∏∞\n",
    "\n",
    "# ÌòïÌÉúÏÜå Î∂ÑÏÑùÍ∏∞ Ï¥àÍ∏∞Ìôî\n",
    "kiwi = Kiwi()\n",
    "tokens = kiwi.tokenize(all_text)\n",
    "\n",
    "# Í∞êÏÑ± ÌíàÏÇ¨: VV(ÎèôÏÇ¨), VA(ÌòïÏö©ÏÇ¨), NNG(Î™ÖÏÇ¨), XR(Ïñ¥Í∑º), MAG(Î∂ÄÏÇ¨)\n",
    "senti_pos_tags = [\"VV\", \"VA\", \"NNG\", \"XR\", \"MAG\"]\n",
    "\n",
    "# Í∞êÏÑ± Î∂ÑÏÑù\n",
    "total_tokens = 0\n",
    "senti_scores = []\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "\n",
    "for token in tokens:\n",
    "    word = token.form\n",
    "    tag = token.tag\n",
    "    total_tokens += 1\n",
    "\n",
    "    # ÎèôÏÇ¨/ÌòïÏö©ÏÇ¨: ÏõêÌòï Î≥µÏõê\n",
    "    if tag.startswith(\"VV\") or tag.startswith(\"VA\"):\n",
    "        word += \"Îã§\"\n",
    "\n",
    "    # Í∞êÏÑ±ÏÇ¨Ï†Ñ Îß§Ïπ≠\n",
    "    if tag[:3] in senti_pos_tags or tag in senti_pos_tags:\n",
    "        if word in senti_lookup:\n",
    "            score = senti_lookup[word]\n",
    "            senti_scores.append(score)\n",
    "            if score > 0:\n",
    "                pos_count += 1\n",
    "            elif score < 0:\n",
    "                neg_count += 1\n",
    "\n",
    "# Í≤∞Í≥º Í≥ÑÏÇ∞\n",
    "senti_count = len(senti_scores)\n",
    "avg_score = round(np.mean(senti_scores), 3) if senti_scores else 0.0\n",
    "ratio = round((senti_count / total_tokens * 100), 2) if total_tokens else 0.0\n",
    "\n",
    "# Ï∂úÎ†•\n",
    "print(\"üìä Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ Í∞êÏÑ± Î∂ÑÏÑù Í≤∞Í≥º:\")\n",
    "print(f\"Ï†ÑÏ≤¥Îã®Ïñ¥Ïàò       : {total_tokens}\")\n",
    "print(f\"Í∞êÏÑ±Îã®Ïñ¥Ïàò       : {senti_count}\")\n",
    "print(f\"Í∏çÏ†ïÎã®Ïñ¥Ïàò       : {pos_count}\")\n",
    "print(f\"Î∂ÄÏ†ïÎã®Ïñ¥Ïàò       : {neg_count}\")\n",
    "print(f\"ÌèâÍ∑†Í∞êÏÑ±Ï†êÏàò     : {avg_score}\")\n",
    "print(f\"Í∞êÏÑ±Îã®Ïñ¥ÎπÑÏú® (%) : {ratio}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c078236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word frequency\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from kiwipiepy import Kiwi\n",
    "from collections import Counter\n",
    "\n",
    "# Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "base_dir = r\"C:\\Users\\ÎÖºÎ¨∏\"\n",
    "input_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
    "df = pd.read_excel(input_path, sheet_name=sheet_name)\n",
    "\n",
    "# ÏãúÍ∏∞ Íµ¨Î∂Ñ\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970‚Äì1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980‚Äì1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988‚Äì1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996‚Äì2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008‚Äì2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015‚Äì2023\"\n",
    "    else: return \"Unknown\"\n",
    "\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# ÌòïÌÉúÏÜå Î∂ÑÏÑùÍ∏∞\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# Î∂ÑÏÑùÌï† ÌíàÏÇ¨\n",
    "target_pos = [\"NNG\", \"VV\", \"VA\", \"MAG\"]\n",
    "\n",
    "def analyze_tokens(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    counters = {\n",
    "        \"NNG\": Counter(),\n",
    "        \"VV\": Counter(),\n",
    "        \"VA\": Counter(),\n",
    "        \"MAG\": Counter(),\n",
    "        \"COMBINED\": Counter()\n",
    "    }\n",
    "    for token in tokens:\n",
    "        word = token.form\n",
    "        tag = token.tag\n",
    "        if tag == \"VV\":\n",
    "            word += \"Îã§\"\n",
    "        if tag == \"VA\":\n",
    "            word += \"Îã§\"\n",
    "        if tag in target_pos:\n",
    "            counters[tag][word] += 1\n",
    "            counters[\"COMBINED\"][word] += 1\n",
    "    return counters\n",
    "\n",
    "# === Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ Î∂ÑÏÑù ===\n",
    "full_text = \" \".join(df[\"Î≥∏Î¨∏ÎÇ¥Ïö©\"].astype(str).tolist())\n",
    "total_counts = analyze_tokens(full_text)\n",
    "\n",
    "# Í≤∞Í≥º Ï†ÄÏû•\n",
    "top_combined = total_counts[\"COMBINED\"].most_common(20)\n",
    "top_nouns = total_counts[\"NNG\"].most_common(10)\n",
    "top_verbs = total_counts[\"VV\"].most_common(10)\n",
    "\n",
    "# === Ïã†Î¨∏ÏÇ¨Î≥Ñ / ÏãúÍ∏∞Î≥Ñ ===\n",
    "def grouped_word_counts(df, by=\"Ïã†Î¨∏ÏÇ¨Î™Ö\"):\n",
    "    result = {}\n",
    "    for key, group in df.groupby(by):\n",
    "        text = \" \".join(group[\"Î≥∏Î¨∏ÎÇ¥Ïö©\"].astype(str).tolist())\n",
    "        counts = analyze_tokens(text)\n",
    "        result[key] = {\n",
    "            \"COMBINED\": counts[\"COMBINED\"].most_common(10),\n",
    "            \"NNG\": counts[\"NNG\"].most_common(5),\n",
    "            \"VV\": counts[\"VV\"].most_common(5)\n",
    "        }\n",
    "    return result\n",
    "\n",
    "period_results = grouped_word_counts(df, by=\"Period\")\n",
    "\n",
    "# Ï∂úÎ†• ÏòàÏãú\n",
    "print(\"‚úÖ Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ Top 20:\")\n",
    "print(top_combined)\n",
    "\n",
    "print(\"\\n‚úÖ Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ Top 10 Î™ÖÏÇ¨:\")\n",
    "print(top_nouns)\n",
    "\n",
    "print(\"\\n‚úÖ Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ Top 10 ÎèôÏÇ¨:\")\n",
    "print(top_verbs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c26352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ÌûàÌä∏Îßµ, Z-score\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from kiwipiepy import Kiwi\n",
    "from collections import Counter\n",
    "from scipy.stats import zscore\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = 'Malgun Gothic'  # ÏúàÎèÑÏö∞ Í∏∞Î≥∏ ÌïúÍ∏Ä Í∏ÄÍº¥\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False     # ÎßàÏù¥ÎÑàÏä§ Í∏∞Ìò∏ Íπ®Ïßê Î∞©ÏßÄ\n",
    "\n",
    "\n",
    "# -------------------- 1. ÌååÏùº Î∂àÎü¨Ïò§Í∏∞ --------------------\n",
    "base_dir = r\"C:\\Users\\ÎÖºÎ¨∏\"\n",
    "input_file = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "\n",
    "df = pd.read_excel(input_file, sheet_name=sheet_name)\n",
    "\n",
    "# -------------------- 2. ÏãúÍ∏∞ Ïª¨Îüº ÎßåÎì§Í∏∞ --------------------\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970‚Äì1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980‚Äì1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988‚Äì1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996‚Äì2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008‚Äì2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015‚Äì2023\"\n",
    "    else: return \"Unknown\"\n",
    "\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# -------------------- 3. ÌòïÌÉúÏÜå Î∂ÑÏÑùÍ∏∞ --------------------\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# -------------------- 4. ÏãúÍ∏∞Î≥Ñ Îã®Ïñ¥ ÎπàÎèÑ ÏàòÏßë --------------------\n",
    "period_word_counts = {}\n",
    "all_top_words = set()\n",
    "\n",
    "for period, group in df.groupby(\"Period\"):\n",
    "    text = \" \".join(group[\"Î≥∏Î¨∏ÎÇ¥Ïö©\"].astype(str).tolist())\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    counter = Counter()\n",
    "\n",
    "    for token in tokens:\n",
    "        word = token.form\n",
    "        tag = token.tag\n",
    "        if tag.startswith(\"VV\"):  # ÎèôÏÇ¨\n",
    "            word += \"Îã§\"\n",
    "            counter[word] += 1\n",
    "        elif tag.startswith(\"NNG\"):  # Î™ÖÏÇ¨\n",
    "            counter[word] += 1\n",
    "\n",
    "    top_words = counter.most_common(15)\n",
    "    all_top_words.update([word for word, _ in top_words])\n",
    "    period_word_counts[period] = counter\n",
    "\n",
    "# -------------------- 5. ÎπàÎèÑÏàò ÌñâÎ†¨ ÏÉùÏÑ± --------------------\n",
    "top_words_list = sorted(all_top_words)\n",
    "periods = sorted(period_word_counts.keys())\n",
    "freq_matrix = pd.DataFrame(index=periods, columns=top_words_list).fillna(0)\n",
    "\n",
    "for period in periods:\n",
    "    counter = period_word_counts[period]\n",
    "    for word in top_words_list:\n",
    "        freq_matrix.loc[period, word] = counter[word]\n",
    "\n",
    "# -------------------- 6. Z-score Ï†ïÍ∑úÌôî --------------------\n",
    "z_freq_matrix = freq_matrix.apply(zscore, axis=0)\n",
    "\n",
    "# -------------------- 7. ÌûàÌä∏Îßµ ÏãúÍ∞ÅÌôî Î∞è Ï†ÄÏû• --------------------\n",
    "# Z-score ÌûàÌä∏Îßµ\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.heatmap(z_freq_matrix, cmap=\"coolwarm\", center=0, annot=True, fmt=\".2f\")\n",
    "plt.title(\"ÏãúÍ∏∞Î≥Ñ Îã®Ïñ¥ ÏÇ¨Ïö© (z-score Ï†ïÍ∑úÌôî)\", fontsize=14)\n",
    "plt.xlabel(\"Îã®Ïñ¥\", fontsize=12)\n",
    "plt.ylabel(\"ÏãúÍ∏∞\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(base_dir, \"3\", \"zscore_heatmap.png\"), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ÏõêÎ≥∏ ÎπàÎèÑ ÌûàÌä∏Îßµ\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.heatmap(freq_matrix.astype(int), cmap=\"YlGnBu\", annot=True, fmt=\"d\")\n",
    "plt.title(\"ÏãúÍ∏∞Î≥Ñ ÏÉÅÏúÑ Îã®Ïñ¥ ÏÇ¨Ïö© ÌûàÌä∏Îßµ (Î™ÖÏÇ¨ + ÎèôÏÇ¨)\", fontsize=14)\n",
    "plt.xlabel(\"Îã®Ïñ¥\", fontsize=12)\n",
    "plt.ylabel(\"ÏãúÍ∏∞\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(base_dir, \"3\", \"freq_heatmap.png\"), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0228ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from kiwipiepy import Kiwi\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ------------------- 1. Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú -------------------\n",
    "base_dir = r\"C:\\Users\\ÎÖºÎ¨∏\"\n",
    "input_file = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "\n",
    "df = pd.read_excel(input_file, sheet_name=sheet_name)\n",
    "\n",
    "# ------------------- 2. ÏãúÍ∏∞ Îß§Ìïë -------------------\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970‚Äì1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980‚Äì1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988‚Äì1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996‚Äì2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008‚Äì2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015‚Äì2023\"\n",
    "    else: return \"Unknown\"\n",
    "\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# ------------------- 3. ÏãúÍ∏∞Î≥Ñ Ï†ÑÏ≤¥ Î¨∏ÏÑú ÏÉùÏÑ± -------------------\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def extract_tokens(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        tag = token.tag\n",
    "        word = token.form\n",
    "        if tag.startswith(\"VV\"):  # ÎèôÏÇ¨\n",
    "            word += \"Îã§\"\n",
    "            result.append(word)\n",
    "        elif tag.startswith(\"NNG\"):  # Î™ÖÏÇ¨\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "# ÏãúÍ∏∞Î≥Ñ ÌÖçÏä§Ìä∏ Î¶¨Ïä§Ìä∏ ÏÉùÏÑ±\n",
    "period_docs = df.groupby(\"Period\")[\"Î≥∏Î¨∏ÎÇ¥Ïö©\"].apply(lambda texts: \" \".join(texts.astype(str))).to_dict()\n",
    "\n",
    "# ÏãúÍ∏∞Î≥Ñ Î¨∏ÏÑúÏóêÏÑú Î™ÖÏÇ¨+ÎèôÏÇ¨ ÌÜ†ÌÅ∞ Ï∂îÏ∂ú ÌõÑ Îã§Ïãú Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôò\n",
    "period_processed_docs = {}\n",
    "for period, text in period_docs.items():\n",
    "    tokens = extract_tokens(text)\n",
    "    period_processed_docs[period] = \" \".join(tokens)\n",
    "\n",
    "# ------------------- 4. TF-IDF Í≥ÑÏÇ∞ -------------------\n",
    "vectorizer = TfidfVectorizer()\n",
    "corpus = [period_processed_docs[period] for period in sorted(period_processed_docs.keys())]\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# ------------------- 5. Í≤∞Í≥º Ï†ïÎ¶¨ -------------------\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(),\n",
    "                        index=sorted(period_processed_docs.keys()),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# ÏÉÅÏúÑ TF-IDF Îã®Ïñ¥ 15Í∞ú Í∏∞Ï§Ä Îã®Ïñ¥ Î¶¨Ïä§Ìä∏ ÎßåÎì§Í∏∞\n",
    "top_words = set()\n",
    "for period in tfidf_df.index:\n",
    "    top = tfidf_df.loc[period].sort_values(ascending=False).head(15).index.tolist()\n",
    "    top_words.update(top)\n",
    "\n",
    "top_words = sorted(top_words)\n",
    "filtered_tfidf_df = tfidf_df[top_words]\n",
    "\n",
    "# ------------------- 6. ÌûàÌä∏Îßµ ÏãúÍ∞ÅÌôî Î∞è Ï†ÄÏû• -------------------\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.heatmap(filtered_tfidf_df, annot=True, cmap=\"YlGnBu\", fmt=\".2f\")\n",
    "plt.title(\"ÏãúÍ∏∞Î≥Ñ ÏÉÅÏúÑ TF-IDF Îã®Ïñ¥ ÌûàÌä∏Îßµ (Î™ÖÏÇ¨ + ÎèôÏÇ¨)\", fontsize=14)\n",
    "plt.xlabel(\"Îã®Ïñ¥\", fontsize=12)\n",
    "plt.ylabel(\"ÏãúÍ∏∞\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join(base_dir, \"3\", \"tfidf_heatmap.png\"), dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db17da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ÌïòÎã§/ÎØ∏ÌòºÎ™® Î∫Ä TF- IDF, Í∏∞Í∞ÑÎ≥Ñ\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "base_dir = r\"C:\\Users\\ÎÖºÎ¨∏\"\n",
    "input_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
    "df = pd.read_excel(input_path, sheet_name=sheet_name)\n",
    "\n",
    "# ÏãúÍ∏∞ Ï†ïÏùò\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970‚Äì1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980‚Äì1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988‚Äì1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996‚Äì2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008‚Äì2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015‚Äì2023\"\n",
    "    else: return \"Unknown\"\n",
    "\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "\n",
    "# ÌòïÌÉúÏÜå Î∂ÑÏÑùÍ∏∞\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# Ï†ÑÏ≤òÎ¶¨ + Ï†úÏô∏Ìï† Îã®Ïñ¥\n",
    "EXCLUDE_WORDS = {\"ÎØ∏ÌòºÎ™®\", \"ÌïòÎã§\"}\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    return [\n",
    "        (token.form + \"Îã§\" if token.tag.startswith(\"VV\") else token.form)\n",
    "        for token in tokens\n",
    "        if token.tag.startswith((\"NNG\", \"VV\")) and\n",
    "           ((token.form + \"Îã§\") if token.tag.startswith(\"VV\") else token.form) not in EXCLUDE_WORDS\n",
    "    ]\n",
    "\n",
    "# TF-IDF Í≥ÑÏÇ∞ Ìï®Ïàò\n",
    "def compute_tfidf(grouped_texts, top_n=30):\n",
    "    corpus = grouped_texts.apply(lambda x: \" \".join(tokenize_text(\" \".join(x.astype(str)))))\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    tfidf_df = pd.DataFrame(X.toarray(), index=grouped_texts.index, columns=vectorizer.get_feature_names_out())\n",
    "    top_words = tfidf_df.max().sort_values(ascending=False).head(top_n).index.tolist()\n",
    "    return tfidf_df[top_words]\n",
    "\n",
    "# ==================== ÏãúÍ∏∞Î≥Ñ TF-IDF ====================\n",
    "period_tfidf = compute_tfidf(df.groupby(\"Period\")[\"Î≥∏Î¨∏ÎÇ¥Ïö©\"])\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(period_tfidf.T, cmap=\"YlGnBu\", annot=True, fmt=\".2f\", linewidths=0.5, cbar_kws={'label': 'TF-IDF'})\n",
    "plt.title(\"ÏãúÍ∏∞Î≥Ñ TF-IDF ÏÉÅÏúÑ Îã®Ïñ¥ ÎπÑÍµê (ÎØ∏ÌòºÎ™®, ÌïòÎã§ Ï†úÏô∏)\", fontsize=14)\n",
    "plt.xlabel(\"ÏãúÍ∏∞\", fontsize=12)\n",
    "plt.ylabel(\"Îã®Ïñ¥\", fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(base_dir, \"3\", \"ÏãúÍ∏∞Î≥Ñ_TFIDF_ÌûàÌä∏Îßµ_Ï†úÏô∏Îã®Ïñ¥Ï†úÍ±∞.png\"))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dbffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ÏúÑÏóêÍ≤ÉÏùÄ kiwiÌòïÌÉúÏÜå Î∂ÑÏÑù ÏïàÌñàÍ∏∞ ÎïåÎ¨∏Ïóê Îã§Ïãú ÌòïÌÉúÏÜå Î∂ÑÏÑù ÌõÑ TF Î∂ÑÏÑù, ÏãúÍ∏∞Î≥ÑÎ°ú ÎÇòÎàà Îí§ Í∞Å ÏãúÍ∏∞ÎßàÎã§ TF-IDFÎ•º ÏÉàÎ°ú Í≥ÑÏÇ∞Ìïú Í≤É\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# ÌååÏùº Í≤ΩÎ°ú\n",
    "base_dir = r\"C:\\Users\\ÎÖºÎ¨∏\"\n",
    "file_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "# ÏãúÍ∏∞ Î∂ÑÎ•ò\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970‚Äì1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980‚Äì1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988‚Äì1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996‚Äì2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008‚Äì2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015‚Äì2023\"\n",
    "    else: return \"Unknown\"\n",
    "\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period) #Ïó∞ÎèÑ(Year) Ï†ïÎ≥¥Î•º Î∞îÌÉïÏúºÎ°ú 6Í∞úÏùò ÏãúÍ∏∞Î°ú Î∂ÑÎ•òÌï©ÎãàÎã§.\n",
    "\n",
    "\n",
    "# ÌòïÌÉúÏÜå Î∂ÑÏÑùÍ∏∞\n",
    "kiwi = Kiwi()\n",
    "EXCLUDE_WORDS = {\"ÎØ∏ÌòºÎ™®\", \"ÌïòÎã§\"}\n",
    "\n",
    "# ÌÖçÏä§Ìä∏ ÌÜ†ÌÅ∞Ìôî\n",
    "def tokenize_text(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    return [\n",
    "        (token.form + \"Îã§\" if token.tag.startswith(\"VV\") else token.form)\n",
    "        for token in tokens\n",
    "        if token.tag.startswith((\"NNG\", \"VV\")) and\n",
    "           ((token.form + \"Îã§\") if token.tag.startswith(\"VV\") else token.form) not in EXCLUDE_WORDS\n",
    "    ]\n",
    "\n",
    "# Î™ÖÏÇ¨(NNG), ÎèôÏÇ¨(VV)Îßå Ï∂îÏ∂úÌïòÍ≥†, Î∂àÏö©Ïñ¥(Ïòà: ‚ÄòÎØ∏ÌòºÎ™®‚Äô, ‚ÄòÌïòÎã§‚Äô) Ï†úÏô∏.ÎèôÏÇ¨Îäî Í∏∞Î≥∏Ìòï + ‚ÄúÎã§‚Äù ÌòïÌÉúÎ°ú ÎßåÎì¶ (e.g., \"Î≥¥\" ‚Üí \"Î≥¥Îã§\")\n",
    "\n",
    "\n",
    "\n",
    "# TF-IDF Í≥ÑÏÇ∞\n",
    "def compute_tfidf(grouped_df, top_n=30):\n",
    "    labels = []\n",
    "    docs = []\n",
    "    for label, group in grouped_df:\n",
    "        full_text = \" \".join(group[\"Î≥∏Î¨∏ÎÇ¥Ïö©\"].astype(str))\n",
    "        tokens = tokenize_text(full_text)\n",
    "        docs.append(\" \".join(tokens)) #Í∞Å ÏãúÍ∏∞Î≥ÑÎ°ú Í∏∞ÏÇ¨Îì§ÏùÑ Î™®ÏïÑ ÌïòÎÇòÏùò ÌÖçÏä§Ìä∏Î°ú ÎßåÎì§Í≥† TF-IDF Í≥ÑÏÇ∞\n",
    "        labels.append(label)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    tfidf_df = pd.DataFrame(X.toarray(), index=labels, columns=vectorizer.get_feature_names_out())\n",
    "    top_words = tfidf_df.max().sort_values(ascending=False).head(top_n).index.tolist()\n",
    "    return tfidf_df[top_words]\n",
    "\n",
    "# Ïã§Ìñâ\n",
    "period_tfidf = compute_tfidf(df.groupby(\"Period\"))\n",
    "\n",
    "\n",
    "# Ï†ÄÏû•\n",
    "period_tfidf.to_csv(os.path.join(base_dir, \"3\", \"ÌòïÌÉúÏÜåÍ∏∞Î∞ò_ÏãúÍ∏∞Î≥Ñ_TFIDF.csv\"), encoding=\"utf-8-sig\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6623fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ÌûàÌä∏Îßµ\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "\n",
    "# ----------- [1] Ìè∞Ìä∏ ÏÑ§Ï†ï (Windows / Mac / Linux ÎåÄÏùë) -----------\n",
    "try:\n",
    "    matplotlib.rc(\"font\", family=\"Malgun Gothic\")  # Windows\n",
    "except:\n",
    "    try:\n",
    "        matplotlib.rc(\"font\", family=\"AppleGothic\")  # Mac\n",
    "    except:\n",
    "        matplotlib.rc(\"font\", family=\"DejaVu Sans\")  # Linux (ÎåÄÏ≤¥)\n",
    "\n",
    "matplotlib.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# ----------- [2] Í≤ΩÎ°ú ÏÑ§Ï†ï Î∞è Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞ -----------\n",
    "base_dir = r\"C:\\Users\\ÎÖºÎ¨∏\\3\"\n",
    "period_path = os.path.join(base_dir, \"ÌòïÌÉúÏÜåÍ∏∞Î∞ò_ÏãúÍ∏∞Î≥Ñ_TFIDF.csv\")\n",
    "\n",
    "\n",
    "period_tfidf = pd.read_csv(period_path, index_col=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_tfidf_heatmap(tfidf_df, title, figsize=(14, 8), output_file=None):\n",
    "    # Î™®Îì† Í∞íÏù¥ 0Ïù∏ Ïó¥ Ï†úÍ±∞\n",
    "    tfidf_df_clean = tfidf_df.loc[:, (tfidf_df != 0).any(axis=0)].dropna(axis=1, how='all')\n",
    "\n",
    "    # Î™®Îì† Í∞íÏù¥ 0Ïù¥Í±∞ÎÇò NaNÏù¥Î©¥ Ï∂úÎ†• Í±¥ÎÑàÎõ∞Í∏∞\n",
    "    if tfidf_df_clean.shape[1] == 0:\n",
    "        print(f\"‚ö†Ô∏è ÌûàÌä∏Îßµ ÏÉùÎûµÎê®: '{title}'Ïóê ÌëúÏãúÌï† Ïú†Ìö®Ìïú TF-IDF Îã®Ïñ¥ ÏóÜÏùå.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(tfidf_df_clean.T, cmap=\"YlGnBu\", annot=True, fmt=\".2f\",\n",
    "                linewidths=0.5, cbar_kws={'label': 'TF-IDF'}, square=False)\n",
    "\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(\"Î∂ÑÎ•ò\", fontsize=13)\n",
    "    plt.ylabel(\"Îã®Ïñ¥\", fontsize=13)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_file:\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        print(f\"‚úÖ Ï†ÄÏû• ÏôÑÎ£å: {output_file}\")\n",
    "    plt.show()\n",
    "\n",
    "# ----------- [4] ÏãúÍ∞ÅÌôî Ïã§Ìñâ -----------\n",
    "plot_tfidf_heatmap(period_tfidf, \"ÏãúÍ∏∞Î≥Ñ TF-IDF ÏÉÅÏúÑ Îã®Ïñ¥ ÌûàÌä∏Îßµ\",\n",
    "                   output_file=os.path.join(base_dir, \"TFIDF_ÏãúÍ∏∞Î≥Ñ_ÌûàÌä∏Îßµ.png\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5befd08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kiwipiepy scikit-learn pandas matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26f0236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from kiwipiepy import Kiwi\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# ============ ÏÇ¨Ïö©Ïûê ÏÑ§Ï†ï ============\n",
    "base_dir = r\"C:\\Users\\ÎÖºÎ¨∏\"\n",
    "input_file = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "exclude_words = {\"ÎØ∏ÌòºÎ™®\", \"ÌïòÎã§\"}\n",
    "top_n = 15\n",
    "output_file_period = os.path.join(base_dir, \"TF_IDF123_ÏãúÍ∏∞Î≥Ñ.png\")\n",
    "\n",
    "\n",
    "# ============ ÌïúÍ∏Ä Ìè∞Ìä∏ ÏÑ§Ï†ï ============\n",
    "font_path = \"C:/Windows/Fonts/malgun.ttf\"\n",
    "if os.path.exists(font_path):\n",
    "    fm.fontManager.addfont(font_path)\n",
    "    plt.rc(\"font\", family=\"Malgun Gothic\")\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# ============ Îç∞Ïù¥ÌÑ∞ Î°úÎìú ============\n",
    "df = pd.read_excel(input_file, sheet_name=sheet_name)\n",
    "\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970‚Äì1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980‚Äì1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988‚Äì1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996‚Äì2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008‚Äì2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015‚Äì2023\"\n",
    "    else: return \"Unknown\"\n",
    "\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "\n",
    "# ============ ÌòïÌÉúÏÜå Î∂ÑÏÑù Î∞è Ï†ÑÏ≤òÎ¶¨ ============\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    words = []\n",
    "    for token in tokens:\n",
    "        tag = token.tag\n",
    "        word = token.form\n",
    "        if tag.startswith(\"NNG\"):\n",
    "            words.append(word)\n",
    "        elif tag.startswith(\"VV\"):\n",
    "            words.append(word + \"Îã§\")\n",
    "    return [w for w in words if w not in exclude_words]\n",
    "\n",
    "df[\"tokens\"] = df[\"Î≥∏Î¨∏ÎÇ¥Ïö©\"].astype(str).apply(lambda x: \" \".join(tokenize(x)))\n",
    "\n",
    "# ============ TF-IDF Í≥ÑÏÇ∞ Î∞è ÌûàÌä∏Îßµ ============\n",
    "def compute_tfidf_heatmap(df, group_col, title, output_path):\n",
    "    grouped = df.groupby(group_col)[\"tokens\"].apply(lambda x: \" \".join(x))\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(grouped)\n",
    "    tfidf_df = pd.DataFrame(X.toarray(), index=grouped.index, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    top_words = set()\n",
    "    for _, row in tfidf_df.iterrows():\n",
    "        top_words.update(row.sort_values(ascending=False).head(top_n).index)\n",
    "    tfidf_top = tfidf_df[list(top_words)]\n",
    "\n",
    "    if tfidf_top.empty or (tfidf_top.sum(axis=0) == 0).all():\n",
    "        print(f\"‚ö†Ô∏è ÌûàÌä∏Îßµ ÏÉùÎûµÎê®: '{title}'Ïóê ÌëúÏãúÌï† Ïú†Ìö®Ìïú TF-IDF Îã®Ïñ¥ ÏóÜÏùå.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(tfidf_top.T, cmap=\"YlGnBu\", annot=True, fmt=\".2f\",\n",
    "                linewidths=0.5, cbar_kws={\"label\": \"TF-IDF\"})\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(group_col, fontsize=12)\n",
    "    plt.ylabel(\"Îã®Ïñ¥\", fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "# Ïã§Ìñâ\n",
    "compute_tfidf_heatmap(df, \"Period\", \"ÏãúÍ∏∞Î≥Ñ TF-IDF ÏÉÅÏúÑ Îã®Ïñ¥ ÌûàÌä∏Îßµ\", output_file_period)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d30bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-Idf Unigram Ïú†ÏÇ¨ÎèÑ Í∫æÏùÄÏÑ†ÏúºÎ°ú, Ïïà Ïì∞ÎäîÍ±∏Î°ú. \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from kiwipiepy import Kiwi\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# ===== ÏÇ¨Ïö©Ïûê ÏÑ§Ï†ï =====\n",
    "base_dir = r\"C:\\Users\\ÎÖºÎ¨∏\"\n",
    "input_file = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "exclude_words = {\"ÎØ∏ÌòºÎ™®\", \"ÌïòÎã§\"}\n",
    "output_file = os.path.join(base_dir, \"TF_IDF123_Í∑∏ÎûòÌîÑ.png\")\n",
    "\n",
    "# ===== ÌïúÍ∏Ä Ìè∞Ìä∏ ÏÑ§Ï†ï =====\n",
    "font_path = \"C:/Windows/Fonts/malgun.ttf\"\n",
    "if os.path.exists(font_path):\n",
    "    fm.fontManager.addfont(font_path)\n",
    "    plt.rc(\"font\", family=\"Malgun Gothic\")\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# ===== Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞ =====\n",
    "df = pd.read_excel(input_file, sheet_name=sheet_name)\n",
    "\n",
    "# ===== ÏãúÍ∏∞ Ï†ïÏùò Ìï®Ïàò =====\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970‚Äì1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980‚Äì1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988‚Äì1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996‚Äì2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008‚Äì2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015‚Äì2023\"\n",
    "    else: return \"Unknown\"\n",
    "\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# ===== ÌòïÌÉúÏÜå Î∂ÑÏÑù =====\n",
    "kiwi = Kiwi()\n",
    "def tokenize(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    words = []\n",
    "    for token in tokens:\n",
    "        if token.tag.startswith(\"NNG\"):\n",
    "            words.append(token.form)\n",
    "        elif token.tag.startswith(\"VV\"):\n",
    "            words.append(token.form + \"Îã§\")\n",
    "    return [w for w in words if w not in exclude_words]\n",
    "\n",
    "df[\"tokens\"] = df[\"Î≥∏Î¨∏ÎÇ¥Ïö©\"].astype(str).apply(lambda x: \" \".join(tokenize(x)))\n",
    "\n",
    "# ===== ÏãúÍ∏∞Î≥Ñ Î¨∏ÏÑú Î¨∂Í∏∞ =====\n",
    "period_grouped = df.groupby(\"Period\")[\"tokens\"].apply(lambda x: \" \".join(x)).sort_index()\n",
    "\n",
    "# ===== TF-IDF Î≤°ÌÑ∞Ìôî Î∞è Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞ =====\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(period_grouped)\n",
    "cos_sim = cosine_similarity(X)\n",
    "\n",
    "# ===== Ïú†ÏÇ¨ÎèÑ DataFrame =====\n",
    "periods = period_grouped.index.tolist()\n",
    "sim_df = pd.DataFrame(cos_sim, index=periods, columns=periods)\n",
    "\n",
    "# ===== Í∫æÏùÄÏÑ† Í∑∏ÎûòÌîÑ =====\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, period in enumerate(periods):\n",
    "    if i != 0:\n",
    "        plt.plot(periods[:i+1], sim_df.iloc[i, :i+1], marker='o', label=f\"{period}\")\n",
    "\n",
    "plt.title(\"ÏãúÍ∏∞Î≥Ñ TF-IDF Unigram Ïú†ÏÇ¨ÎèÑ ÎπÑÍµê\", fontsize=14)\n",
    "plt.ylabel(\"Cosine Similarity\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Í∏∞Ï§Ä ÏãúÍ∏∞\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_file)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8360c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#axis = 1 Î°ú ÎßåÎì§Ïñ¥ÏÑú ÌïòÎÇòÏùò Î¨∏ÏÑú ÏïàÏóê Îì±Ïû•Ìïú Î™®Îì† Îã®Ïñ¥Îì§Ïùò TF-IDF Í∞ÄÏ§ëÏπò Ìï© ÏùÑ Íµ¨ÌïúÍ±∞ÏûÑ. ÏûòÎ™ªÎêòÏÑú Î∞ëÏóêÏÑú Îã§Ïãú Íµ¨Ìï†„Ñπ„ÖïÍ≥†\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from kiwipiepy import Kiwi\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ===== ÏÇ¨Ïö©Ïûê Í≤ΩÎ°ú ÏÑ§Ï†ï =====\n",
    "base_dir = r\"C:\\Users\\ÎÖºÎ¨∏\"\n",
    "input_file = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "exclude_words = {\"ÎØ∏ÌòºÎ™®\", \"ÌïòÎã§\"}\n",
    "\n",
    "# ===== Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞ =====\n",
    "df = pd.read_excel(input_file, sheet_name=sheet_name)\n",
    "\n",
    "# ===== ÌòïÌÉúÏÜå Î∂ÑÏÑù Î∞è Ï†ÑÏ≤òÎ¶¨ =====\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    words = []\n",
    "    for token in tokens:\n",
    "        if token.tag.startswith(\"NNG\"):\n",
    "            words.append(token.form)\n",
    "        elif token.tag.startswith(\"VV\"):\n",
    "            words.append(token.form + \"Îã§\")\n",
    "    return [w for w in words if w not in exclude_words]\n",
    "\n",
    "df[\"tokens\"] = df[\"Î≥∏Î¨∏ÎÇ¥Ïö©\"].astype(str).apply(lambda x: \" \".join(tokenize(x)))\n",
    "\n",
    "# ===== TF-IDF Í≥ÑÏÇ∞ =====\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"tokens\"])\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# ===== TF-IDF Ìï©ÏÇ∞ (Î¨∏ÏÑúÎ≥Ñ) =====\n",
    "df[\"sum_tfidf\"] = tfidf_df.sum(axis=1)\n",
    "\n",
    "# ===== Í∏∞Ïà†ÌÜµÍ≥Ñ ÏöîÏïΩ =====\n",
    "stats = df[\"sum_tfidf\"].describe(percentiles=[.25, .5, .75]).round(3)\n",
    "stats_df = pd.DataFrame(stats).T\n",
    "stats_df.index = [\"sumTF-IDF\"]\n",
    "stats_df = stats_df[[\"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"]]\n",
    "\n",
    "# ===== Í≤∞Í≥º Ï∂úÎ†• =====\n",
    "print(\"üìä TF-IDF Í∞ÄÏ§ëÏπò Ï¥ùÌï© Í∏∞Ïà†ÌÜµÍ≥Ñ:\")\n",
    "print(stats_df)\n",
    "\n",
    "# ===== ÏÑ†ÌÉù: LaTeX ÌÖåÏù¥Î∏îÎ°ú Ï†ÄÏû• =====\n",
    "latex_output_path = os.path.join(base_dir, \"TFIDF_sum_stats.tex\")\n",
    "with open(latex_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(stats_df.to_latex(index=True, caption=\"TF-IDF Í∞ÄÏ§ëÏπò Ï¥ùÌï© Í∏∞Ïà†ÌÜµÍ≥Ñ\", label=\"tab:tfidf_stats\"))\n",
    "\n",
    "print(f\"\\nüìÑ LaTeX ÌÖåÏù¥Î∏î Ï†ÄÏû• ÏôÑÎ£å: {latex_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa97b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ÏïÑÍπå Íµ¨Ìïú TF-IDF ÏóëÏÖÄÎ°ú ÌëúÎ°ú ÎßåÎì§Î†§Í≥†.\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from kiwipiepy import Kiwi\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# ============ ÏÇ¨Ïö©Ïûê ÏÑ§Ï†ï ============ \n",
    "base_dir = r\"C:\\Users\\ÎÖºÎ¨∏\"\n",
    "input_file = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "exclude_words = {\"ÎØ∏ÌòºÎ™®\", \"ÌïòÎã§\"}\n",
    "top_n = 15\n",
    "output_file_period = os.path.join(base_dir, \"TF_IDF123_ÏãúÍ∏∞Î≥Ñ.xlsx\")\n",
    "\n",
    "\n",
    "# ============ ÌïúÍ∏Ä Ìè∞Ìä∏ ÏÑ§Ï†ï ============ \n",
    "font_path = \"C:/Windows/Fonts/malgun.ttf\"\n",
    "if os.path.exists(font_path):\n",
    "    fm.fontManager.addfont(font_path)\n",
    "    plt.rc(\"font\", family=\"Malgun Gothic\")\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# ============ Îç∞Ïù¥ÌÑ∞ Î°úÎìú ============ \n",
    "df = pd.read_excel(input_file, sheet_name=sheet_name)\n",
    "\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970‚Äì1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980‚Äì1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988‚Äì1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996‚Äì2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008‚Äì2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015‚Äì2023\"\n",
    "    else: return \"Unknown\"\n",
    "\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "\n",
    "# ============ ÌòïÌÉúÏÜå Î∂ÑÏÑù Î∞è Ï†ÑÏ≤òÎ¶¨ ============ \n",
    "kiwi = Kiwi()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    words = []\n",
    "    for token in tokens:\n",
    "        tag = token.tag\n",
    "        word = token.form\n",
    "        if tag.startswith(\"NNG\"):\n",
    "            words.append(word)\n",
    "        elif tag.startswith(\"VV\"):\n",
    "            words.append(word + \"Îã§\")\n",
    "    return [w for w in words if w not in exclude_words]\n",
    "\n",
    "df[\"tokens\"] = df[\"Î≥∏Î¨∏ÎÇ¥Ïö©\"].astype(str).apply(lambda x: \" \".join(tokenize(x)))\n",
    "\n",
    "# ============ TF-IDF Í≥ÑÏÇ∞ Î∞è ÏóëÏÖÄÎ°ú Ï†ÄÏû• ============ \n",
    "def compute_tfidf_to_excel(df, group_col, output_path):\n",
    "    grouped = df.groupby(group_col)[\"tokens\"].apply(lambda x: \" \".join(x))\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(grouped)\n",
    "    tfidf_df = pd.DataFrame(X.toarray(), index=grouped.index, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    top_words = set()\n",
    "    for _, row in tfidf_df.iterrows():\n",
    "        top_words.update(row.sort_values(ascending=False).head(top_n).index)\n",
    "    \n",
    "    tfidf_top = tfidf_df[list(top_words)]\n",
    "\n",
    "    # ÏóëÏÖÄ ÌååÏùºÎ°ú Ï†ÄÏû•\n",
    "    if not tfidf_top.empty:\n",
    "        with pd.ExcelWriter(output_path) as writer:\n",
    "            tfidf_top.to_excel(writer, sheet_name=group_col)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è ÏóëÏÖÄÎ°ú Ï†ÄÏû•ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§: '{group_col}'Ïóê Ïú†Ìö®Ìïú TF-IDF Îã®Ïñ¥Í∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
    "\n",
    "# Ïã§Ìñâ\n",
    "compute_tfidf_to_excel(df, \"Period\", output_file_period)\n",
    "\n",
    "\n",
    "print(\"TF-IDF Í≤∞Í≥ºÍ∞Ä ÏóëÏÖÄ ÌååÏùºÎ°ú Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a10d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from kiwipiepy import Kiwi\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# ============ ÏÇ¨Ïö©Ïûê ÏÑ§Ï†ï ============ \n",
    "base_dir = r\"C:\\Users\\ÎÖºÎ¨∏\"\n",
    "input_file = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "exclude_words = {\"ÎØ∏ÌòºÎ™®\", \"ÌïòÎã§\"}\n",
    "top_n = 10  # top 10 Îã®Ïñ¥Îßå\n",
    "\n",
    "# ============ ÌïúÍ∏Ä Ìè∞Ìä∏ ÏÑ§Ï†ï ============ \n",
    "font_path = \"C:/Windows/Fonts/malgun.ttf\"\n",
    "if os.path.exists(font_path):\n",
    "    fm.fontManager.addfont(font_path)\n",
    "    plt.rc(\"font\", family=\"Malgun Gothic\")\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# ============ Îç∞Ïù¥ÌÑ∞ Î°úÎìú ============ \n",
    "df = pd.read_excel(input_file, sheet_name=sheet_name)\n",
    "\n",
    "# ============ ÌòïÌÉúÏÜå Î∂ÑÏÑù Î∞è Ï†ÑÏ≤òÎ¶¨ ============ \n",
    "kiwi = Kiwi()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    words = []\n",
    "    for token in tokens:\n",
    "        tag = token.tag\n",
    "        word = token.form\n",
    "        if tag.startswith(\"NNG\"):\n",
    "            words.append(word)\n",
    "        elif tag.startswith(\"VV\"):\n",
    "            words.append(word + \"Îã§\")\n",
    "    return [w for w in words if w not in exclude_words]\n",
    "\n",
    "df[\"tokens\"] = df[\"Î≥∏Î¨∏ÎÇ¥Ïö©\"].astype(str).apply(lambda x: \" \".join(tokenize(x)))\n",
    "\n",
    "# ============ TF-IDF Í≥ÑÏÇ∞ ============ \n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"tokens\"])\n",
    "\n",
    "# TF-IDF Í∞íÏùÑ Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏúºÎ°ú Î≥ÄÌôò\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Í∞Å Îã®Ïñ¥Ïóê ÎåÄÌï¥ TF-IDF Í∞íÏùò Ï¥ùÌï©ÏùÑ Í≥ÑÏÇ∞\n",
    "tfidf_sums = tfidf_df.sum(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# ÏÉÅÏúÑ 10Í∞ú TF-IDF Ï¥ùÌï©Ïù¥ ÎÜíÏùÄ Îã®Ïñ¥ÏôÄ ÌïòÏúÑ 10Í∞ú TF-IDF Ï¥ùÌï©Ïù¥ ÎÇÆÏùÄ Îã®Ïñ¥ Ï∂îÏ∂ú\n",
    "top_words = tfidf_sums.head(top_n)\n",
    "bottom_words = tfidf_sums.tail(top_n)\n",
    "\n",
    "# Í≤∞Í≥º Ï∂úÎ†•\n",
    "print(\"Top 10 TF-IDF Í∞íÏù¥ ÎÜíÏùÄ Îã®Ïñ¥Îì§:\")\n",
    "print(top_words)\n",
    "\n",
    "print(\"\\nTop 10 TF-IDF Í∞íÏù¥ ÎÇÆÏùÄ Îã®Ïñ¥Îì§:\")\n",
    "print(bottom_words)\n",
    "\n",
    "# ============ ÏãúÍ∞ÅÌôî (TF-IDF Ï¥ùÌï©) ============ \n",
    "def plot_tfidf_words(tfidf_series, title):\n",
    "    # ÏãúÍ∞ÅÌôî\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    tfidf_series.plot(kind='barh', color='skyblue')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(\"total TF-IDF Weight\", fontsize=12)\n",
    "    plt.ylabel(\"words\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ÎÜíÏùÄ TF-IDF Í∞í Îã®Ïñ¥ ÏãúÍ∞ÅÌôî\n",
    "plot_tfidf_words(top_words, \"Top 10 TF-IDF Í∞íÏù¥ ÎÜíÏùÄ Îã®Ïñ¥Îì§\")\n",
    "\n",
    "# ÎÇÆÏùÄ TF-IDF Í∞í Îã®Ïñ¥ ÏãúÍ∞ÅÌôî\n",
    "plot_tfidf_words(bottom_words, \"Top 10 TF-IDF Í∞íÏù¥ ÎÇÆÏùÄ Îã®Ïñ¥Îì§\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c413c9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ÏãúÍ∏∞Î≥Ñ  sum TF-IDFÎäî Ïó¨Îü¨ Î¨∏ÏÑúÏóêÏÑú Îì±Ïû•Ìï¥Ïïº ÎÜíÏïÑÏßÄÎØÄÎ°ú Î∂ÑÏÇ∞Îêú Ï§ëÏöîÎèÑÎ•º Ïûò Î∞òÏòÅÌï®. Í∑∏ÎûòÏÑú ÏïÑÍπå Ìïú ÏãúÍ∏∞Î≥Ñ TF-IDF, Ïã†Î¨∏ÏÇ¨Î≥Ñ TF-IDF Ïô∏Ïóê Ïù¥Í±∏ Îã§Ïãú\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# ========= ÏÑ§Ï†ï =========\n",
    "base_dir = r\"C:\\Users\\ÎÖºÎ¨∏\"\n",
    "file_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "EXCLUDE_WORDS = {\"ÎØ∏ÌòºÎ™®\", \"ÌïòÎã§\"}\n",
    "TOP_N = 20\n",
    "\n",
    "# ========= Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞ =========\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "# ========= ÏãúÍ∏∞ Î∂ÑÎ•ò =========\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970‚Äì1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980‚Äì1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988‚Äì1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996‚Äì2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008‚Äì2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015‚Äì2023\"\n",
    "    else: return \"Unknown\"\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# ========= ÌòïÌÉúÏÜå Î∂ÑÏÑùÍ∏∞ & ÌÜ†ÌÅ∞Ìôî =========\n",
    "kiwi = Kiwi()\n",
    "def tokenize_text(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    return [\n",
    "        (token.form + \"Îã§\" if token.tag.startswith(\"VV\") else token.form)\n",
    "        for token in tokens\n",
    "        if token.tag.startswith((\"NNG\", \"VV\")) and\n",
    "           ((token.form + \"Îã§\") if token.tag.startswith(\"VV\") else token.form) not in EXCLUDE_WORDS\n",
    "    ]\n",
    "\n",
    "# ========= ÏãúÍ∏∞Î≥Ñ Î¨∏ÏÑú ÏßëÍ≥Ñ =========\n",
    "period_docs = defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    period = row[\"Period\"]\n",
    "    tokens = tokenize_text(str(row[\"Î≥∏Î¨∏ÎÇ¥Ïö©\"]))\n",
    "    period_docs[period].append(\" \".join(tokens))\n",
    "\n",
    "# ========= Ï†ÑÏ≤¥ TF-IDF Í≥ÑÏÇ∞ ‚Üí Top 20 Îã®Ïñ¥ ÏÑ†Ï†ï =========\n",
    "all_docs = []\n",
    "all_labels = []\n",
    "for period, docs in period_docs.items():\n",
    "    all_docs.extend(docs)\n",
    "    all_labels.extend([period] * len(docs))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_all = vectorizer.fit_transform(all_docs)\n",
    "df_all = pd.DataFrame(X_all.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "total_sum = df_all.sum(axis=0).sort_values(ascending=False)\n",
    "top_words = total_sum.head(TOP_N).index.tolist()\n",
    "\n",
    "# ÏãúÍ∏∞Î≥Ñ sum TF-IDF (Top 20 Îã®Ïñ¥Îßå)\n",
    "period_sums = {}\n",
    "\n",
    "for period, docs in period_docs.items():\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # ÎàÑÎùΩÎêú Îã®Ïñ¥Îäî 0ÏúºÎ°ú Ï∂îÍ∞Ä\n",
    "    for word in top_words:\n",
    "        if word not in df_tfidf.columns:\n",
    "            df_tfidf[word] = 0.0\n",
    "\n",
    "    # top_words ÏàúÏÑúÎåÄÎ°ú Ï†ïÎ†¨\n",
    "    sum_tfidf = df_tfidf[top_words].sum(axis=0)\n",
    "    period_sums[period] = sum_tfidf\n",
    "\n",
    "# ========= ÏãúÍ∏∞Î≥Ñ sum TF-IDF (Top 20 Îã®Ïñ¥Îßå) =========\n",
    "period_sums = {}\n",
    "for period, docs in period_docs.items():\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    sum_tfidf = df_tfidf[top_words].sum(axis=0)\n",
    "    period_sums[period] = sum_tfidf\n",
    "\n",
    "# ========= Í≤∞Í≥º Ï†ïÎ¶¨ =========\n",
    "result_df = pd.DataFrame(period_sums).T.fillna(0)\n",
    "output_path = os.path.join(base_dir, \"3\", \"ÏãúÍ∏∞Î≥Ñ_top20_sumTFIDF.csv\")\n",
    "result_df.to_csv(output_path, encoding=\"utf-8-sig\")\n",
    "print(f\"‚úÖ Ï†ÄÏû• ÏôÑÎ£å: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b56650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ÏãúÍ∞ÅÌôî\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ÌïúÍ∏Ä Ìè∞Ìä∏ ÏÑ§Ï†ï (Windows Í∏∞Ï§Ä)\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ========== 1. Í∫æÏùÄÏÑ† Í∑∏ÎûòÌîÑ ==========\n",
    "plt.figure(figsize=(12, 6))\n",
    "for column in result_df.columns:\n",
    "    plt.plot(result_df.index, result_df[column], marker='o', label=column)\n",
    "\n",
    "plt.title(\"ÏãúÍ∏∞Î≥Ñ Top 20 Îã®Ïñ¥Ïùò sum TF-IDF Î≥ÄÌôî\")\n",
    "plt.xlabel(\"ÏãúÍ∏∞\")\n",
    "plt.ylabel(\"sum TF-IDF\")\n",
    "plt.legend(loc=\"upper right\", bbox_to_anchor=(1.15, 1))\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========== 2. ÌûàÌä∏Îßµ ==========\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(result_df, annot=True, fmt=\".2f\", cmap=\"YlGnBu\")\n",
    "plt.title(\"ÏãúÍ∏∞Î≥Ñ Top 20 Îã®Ïñ¥Ïùò sum TF-IDF ÌûàÌä∏Îßµ\")\n",
    "plt.xlabel(\"Îã®Ïñ¥\")\n",
    "plt.ylabel(\"ÏãúÍ∏∞\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1b39a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# ========== ÏÑ§Ï†ï ==========\n",
    "base_dir = r\"C:\\Users\\ÎÖºÎ¨∏\"\n",
    "file_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "EXCLUDE_WORDS = {\"ÎØ∏ÌòºÎ™®\", \"ÌïòÎã§\"}\n",
    "TOP_N = 20\n",
    "\n",
    "# ========== Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞ ==========\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "# ========== ÏãúÍ∏∞ Î∂ÑÎ•ò ==========\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970‚Äì1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980‚Äì1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988‚Äì1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996‚Äì2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008‚Äì2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015‚Äì2023\"\n",
    "    else: return \"Unknown\"\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# ========== ÌòïÌÉúÏÜå Î∂ÑÏÑùÍ∏∞ ==========\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    return [\n",
    "        (token.form + \"Îã§\" if token.tag.startswith(\"VV\") else token.form)\n",
    "        for token in tokens\n",
    "        if token.tag.startswith((\"NNG\", \"VV\")) and\n",
    "           ((token.form + \"Îã§\") if token.tag.startswith(\"VV\") else token.form) not in EXCLUDE_WORDS\n",
    "    ]\n",
    "\n",
    "# ========== ÏãúÍ∏∞Î≥Ñ Î¨∏ÏÑú ÌÜ†ÌÅ∞Ìôî ==========\n",
    "period_docs = defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    period = row[\"Period\"]\n",
    "    tokens = tokenize_text(str(row[\"Î≥∏Î¨∏ÎÇ¥Ïö©\"]))\n",
    "    period_docs[period].append(\" \".join(tokens))\n",
    "\n",
    "# ========== Ï†ÑÏ≤¥ TF-IDF Í≥ÑÏÇ∞ ‚Üí Top N Îã®Ïñ¥ Ï∂îÏ∂ú ==========\n",
    "all_docs = []\n",
    "for docs in period_docs.values():\n",
    "    all_docs.extend(docs)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_all = vectorizer.fit_transform(all_docs)\n",
    "df_all = pd.DataFrame(X_all.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "top_words = df_all.sum().sort_values(ascending=False).head(TOP_N).index.tolist()\n",
    "\n",
    "# ========== ÏãúÍ∏∞Î≥Ñ mean TF-IDF Í≥ÑÏÇ∞ ==========\n",
    "period_mean = {}\n",
    "for period, docs in period_docs.items():\n",
    "    X = vectorizer.transform(docs)\n",
    "    df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # ÎàÑÎùΩ Îã®Ïñ¥ Î≥¥Ï†ï\n",
    "    for word in top_words:\n",
    "        if word not in df_tfidf.columns:\n",
    "            df_tfidf[word] = 0.0\n",
    "\n",
    "    # ÌèâÍ∑† TF-IDF\n",
    "    mean_tfidf = df_tfidf[top_words].mean(axis=0)\n",
    "    period_mean[period] = mean_tfidf\n",
    "\n",
    "# ========== Í≤∞Í≥º Ï†ïÎ¶¨ ==========\n",
    "mean_tfidf_df = pd.DataFrame(period_mean).T[top_words]  # Period x Top 20 Îã®Ïñ¥\n",
    "output_path = os.path.join(base_dir, \"3\", \"ÏãúÍ∏∞Î≥Ñ_top20_meanTFIDF.csv\")\n",
    "mean_tfidf_df.to_csv(output_path, encoding=\"utf-8-sig\")\n",
    "print(f\"‚úÖ ÏãúÍ∏∞Î≥Ñ ÌèâÍ∑† TF-IDF Í≤∞Í≥º Ï†ÄÏû• ÏôÑÎ£å: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25837a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ÌïúÍ∏Ä Ìè∞Ìä∏ ÏÑ§Ï†ï (Windows Í∏∞Ï§Ä)\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ========== 1. Í∫æÏùÄÏÑ† Í∑∏ÎûòÌîÑ ==========\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for word in mean_tfidf_df.columns:\n",
    "    plt.plot(mean_tfidf_df.index, mean_tfidf_df[word], marker='o', label=word)\n",
    "\n",
    "plt.title(\"ÏãúÍ∏∞Î≥Ñ Top 20 Îã®Ïñ¥Ïùò ÌèâÍ∑† TF-IDF Î≥ÄÌôî\")\n",
    "plt.xlabel(\"ÏãúÍ∏∞\")\n",
    "plt.ylabel(\"Mean TF-IDF\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.15, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========== 2. ÌûàÌä∏Îßµ ==========\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(mean_tfidf_df, annot=True, fmt=\".4f\", cmap=\"YlGnBu\")\n",
    "\n",
    "plt.title(\"ÏãúÍ∏∞Î≥Ñ Top 20 Îã®Ïñ¥Ïùò ÌèâÍ∑† TF-IDF ÌûàÌä∏Îßµ\")\n",
    "plt.xlabel(\"Îã®Ïñ¥\")\n",
    "plt.ylabel(\"ÏãúÍ∏∞\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a4a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#z-scoreÎ°ú Ï§ëÏöîÎèÑ Î∂ÑÏÑù (Z-score Í∞í Í∏∞Î∞ò Îã®Ïñ¥Ïùò ÏÉÅÎåÄ Ï§ëÏöîÎèÑ), mean tf-idfÏùò z-score\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# ========== ÏÑ§Ï†ï ==========\n",
    "base_dir = r\"C:\\Users\\ÎÖºÎ¨∏\"\n",
    "file_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "EXCLUDE_WORDS = {\"ÎØ∏ÌòºÎ™®\", \"ÌïòÎã§\"}\n",
    "TOP_N = 20\n",
    "\n",
    "# ========== Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞ ==========\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "# ========== ÏãúÍ∏∞ Î∂ÑÎ•ò ==========\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970‚Äì1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980‚Äì1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988‚Äì1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996‚Äì2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008‚Äì2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015‚Äì2023\"\n",
    "    else: return \"Unknown\"\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# ========== ÌòïÌÉúÏÜå Î∂ÑÏÑùÍ∏∞ ==========\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    return [\n",
    "        (token.form + \"Îã§\" if token.tag.startswith(\"VV\") else token.form)\n",
    "        for token in tokens\n",
    "        if token.tag.startswith((\"NNG\", \"VV\")) and\n",
    "           ((token.form + \"Îã§\") if token.tag.startswith(\"VV\") else token.form) not in EXCLUDE_WORDS\n",
    "    ]\n",
    "\n",
    "# ========== ÏãúÍ∏∞Î≥Ñ Î¨∏ÏÑú ÌÜ†ÌÅ∞Ìôî ==========\n",
    "period_docs = defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    period = row[\"Period\"]\n",
    "    tokens = tokenize_text(str(row[\"Î≥∏Î¨∏ÎÇ¥Ïö©\"]))\n",
    "    period_docs[period].append(\" \".join(tokens))\n",
    "\n",
    "# ========== Ï†ÑÏ≤¥ TF-IDF Í≥ÑÏÇ∞ ‚Üí Top N Îã®Ïñ¥ Ï∂îÏ∂ú ==========\n",
    "all_docs = []\n",
    "for docs in period_docs.values():\n",
    "    all_docs.extend(docs)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_all = vectorizer.fit_transform(all_docs)\n",
    "df_all = pd.DataFrame(X_all.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "top_words = df_all.sum().sort_values(ascending=False).head(TOP_N).index.tolist()\n",
    "\n",
    "# ========== ÏãúÍ∏∞Î≥Ñ mean TF-IDF Í≥ÑÏÇ∞ ==========\n",
    "period_mean = {}\n",
    "for period, docs in period_docs.items():\n",
    "    X = vectorizer.transform(docs)\n",
    "    df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # ÎàÑÎùΩ Îã®Ïñ¥ Î≥¥Ï†ï\n",
    "    for word in top_words:\n",
    "        if word not in df_tfidf.columns:\n",
    "            df_tfidf[word] = 0.0\n",
    "\n",
    "    # ÌèâÍ∑† TF-IDF\n",
    "    mean_tfidf = df_tfidf[top_words].mean(axis=0)\n",
    "    period_mean[period] = mean_tfidf\n",
    "\n",
    "# ========== Í≤∞Í≥º Ï†ïÎ¶¨ ==========\n",
    "mean_tfidf_df = pd.DataFrame(period_mean).T[top_words]  # Period x Top 20 Îã®Ïñ¥\n",
    "\n",
    "# ========== Z-score Í≥ÑÏÇ∞ ==========\n",
    "mean_val = mean_tfidf_df.values.mean()\n",
    "std_val = mean_tfidf_df.values.std()\n",
    "z_score_df = (mean_tfidf_df - mean_val) / std_val\n",
    "\n",
    "# ========== Ï†ÄÏû• ==========\n",
    "output_dir = os.path.join(base_dir, \"3\")\n",
    "z_out = os.path.join(output_dir, \"ÏãúÍ∏∞Î≥Ñ_top20_zscore.csv\")\n",
    "\n",
    "mean_tfidf_df.to_csv(mean_out, encoding=\"utf-8-sig\")\n",
    "z_score_df.to_csv(z_out, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Z-score Ï†ÄÏû•: {z_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1983d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum tf-idf ÏóêÏÑú z-scoreÍπåÏßÄ Íµ¨ÌïòÍ∏∞\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# ========== ÏÑ§Ï†ï ==========\n",
    "base_dir = r\"C:\\Users\\ÎÖºÎ¨∏\"\n",
    "file_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "EXCLUDE_WORDS = {\"ÎØ∏ÌòºÎ™®\", \"ÌïòÎã§\"}\n",
    "TOP_N = 20\n",
    "\n",
    "# ========== Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞ ==========\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "# ========== ÏãúÍ∏∞ Î∂ÑÎ•ò ==========\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970‚Äì1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980‚Äì1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988‚Äì1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996‚Äì2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008‚Äì2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015‚Äì2023\"\n",
    "    else: return \"Unknown\"\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# ========== ÌòïÌÉúÏÜå Î∂ÑÏÑùÍ∏∞ ==========\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    return [\n",
    "        (token.form + \"Îã§\" if token.tag.startswith(\"VV\") else token.form)\n",
    "        for token in tokens\n",
    "        if token.tag.startswith((\"NNG\", \"VV\")) and\n",
    "           ((token.form + \"Îã§\") if token.tag.startswith(\"VV\") else token.form) not in EXCLUDE_WORDS\n",
    "    ]\n",
    "\n",
    "# ========== ÏãúÍ∏∞Î≥Ñ Î¨∏ÏÑú ÌÜ†ÌÅ∞Ìôî ==========\n",
    "period_docs = defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    period = row[\"Period\"]\n",
    "    tokens = tokenize_text(str(row[\"Î≥∏Î¨∏ÎÇ¥Ïö©\"]))\n",
    "    period_docs[period].append(\" \".join(tokens))\n",
    "\n",
    "# ========== Ï†ÑÏ≤¥ TF-IDF Í≥ÑÏÇ∞ ‚Üí Top N Îã®Ïñ¥ Ï∂îÏ∂ú ==========\n",
    "all_docs = []\n",
    "for docs in period_docs.values():\n",
    "    all_docs.extend(docs)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_all = vectorizer.fit_transform(all_docs)\n",
    "df_all = pd.DataFrame(X_all.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "top_words = df_all.sum().sort_values(ascending=False).head(TOP_N).index.tolist()\n",
    "\n",
    "# ========== ÏãúÍ∏∞Î≥Ñ sum TF-IDF Í≥ÑÏÇ∞ ==========\n",
    "period_sum = {}\n",
    "for period, docs in period_docs.items():\n",
    "    X = vectorizer.transform(docs)\n",
    "    df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # ÎàÑÎùΩÎêú Îã®Ïñ¥Îäî 0ÏúºÎ°ú Ï±ÑÏö∞Í∏∞\n",
    "    for word in top_words:\n",
    "        if word not in df_tfidf.columns:\n",
    "            df_tfidf[word] = 0.0\n",
    "\n",
    "    # sum TF-IDF\n",
    "    sum_tfidf = df_tfidf[top_words].sum(axis=0)\n",
    "    period_sum[period] = sum_tfidf\n",
    "\n",
    "sum_tfidf_df = pd.DataFrame(period_sum).T[top_words]\n",
    "\n",
    "# ========== Z-score Ï†ïÍ∑úÌôî ==========\n",
    "mean_val = sum_tfidf_df.values.mean()\n",
    "std_val = sum_tfidf_df.values.std()\n",
    "z_score_sum_df = (sum_tfidf_df - mean_val) / std_val\n",
    "\n",
    "# ========== Ï†ÄÏû• ==========\n",
    "output_dir = os.path.join(base_dir, \"3\")\n",
    "sum_out = os.path.join(output_dir, \"ÏãúÍ∏∞Î≥Ñ_top20_sumTFIDF.csv\")\n",
    "z_out = os.path.join(output_dir, \"ÏãúÍ∏∞Î≥Ñ_top20_sum_zscore.csv\")\n",
    "\n",
    "# Ïó¥Î†§ÏûàÎäî ÌååÏùº Î∞©ÏßÄ ‚Üí ÌååÏùºÎ™Ö Î∞îÍøî Ï†ÄÏû• ÏãúÎèÑ\n",
    "try:\n",
    "    sum_tfidf_df.to_csv(sum_out, encoding=\"utf-8-sig\")\n",
    "    z_score_sum_df.to_csv(z_out, encoding=\"utf-8-sig\")\n",
    "    print(f\"‚úÖ sum TF-IDF Ï†ÄÏû• ÏôÑÎ£å: {sum_out}\")\n",
    "    print(f\"‚úÖ Z-score Ï†ÄÏû• ÏôÑÎ£å: {z_out}\")\n",
    "except PermissionError:\n",
    "    alt_sum_out = os.path.join(output_dir, \"ÏãúÍ∏∞Î≥Ñ_top20_sumTFIDF_v2.csv\")\n",
    "    alt_z_out = os.path.join(output_dir, \"ÏãúÍ∏∞Î≥Ñ_top20_sum_zscore_v2.csv\")\n",
    "    sum_tfidf_df.to_csv(alt_sum_out, encoding=\"utf-8-sig\")\n",
    "    z_score_sum_df.to_csv(alt_z_out, encoding=\"utf-8-sig\")\n",
    "    print(f\"‚ö†Ô∏è Í∏∞Ï°¥ ÌååÏùº Ïó¥Î†§ ÏûàÏñ¥ Îã§Î•∏ Ïù¥Î¶ÑÏúºÎ°ú Ï†ÄÏû•Ìï®:\")\n",
    "    print(f\"‚úîÔ∏è sum TF-IDF ‚Üí {alt_sum_out}\")\n",
    "    print(f\"‚úîÔ∏è Z-score ‚Üí {alt_z_out}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f79ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# ========== ÏÑ§Ï†ï ==========\n",
    "base_dir = r\"C:\\Users\\ÎÖºÎ¨∏\"\n",
    "file_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "EXCLUDE_WORDS = {\"ÎØ∏ÌòºÎ™®\", \"ÌïòÎã§\"}\n",
    "TOP_N = 20\n",
    "\n",
    "# ========== Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞ ==========\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "# ========== ÏãúÍ∏∞ Î∂ÑÎ•ò ==========\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970‚Äì1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980‚Äì1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988‚Äì1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996‚Äì2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008‚Äì2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015‚Äì2023\"\n",
    "    else: return \"Unknown\"\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# ========== ÌòïÌÉúÏÜå Î∂ÑÏÑùÍ∏∞ ==========\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    return [\n",
    "        (token.form + \"Îã§\" if token.tag.startswith(\"VV\") else token.form)\n",
    "        for token in tokens\n",
    "        if token.tag.startswith((\"NNG\", \"VV\")) and\n",
    "           ((token.form + \"Îã§\") if token.tag.startswith(\"VV\") else token.form) not in EXCLUDE_WORDS\n",
    "    ]\n",
    "\n",
    "# ========== ÏãúÍ∏∞Î≥Ñ Î¨∏ÏÑú ÌÜ†ÌÅ∞Ìôî ==========\n",
    "period_docs = defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    period = row[\"Period\"]\n",
    "    tokens = tokenize_text(str(row[\"Î≥∏Î¨∏ÎÇ¥Ïö©\"]))\n",
    "    period_docs[period].append(\" \".join(tokens))\n",
    "\n",
    "# ========== Ï†ÑÏ≤¥ TF-IDF Í≥ÑÏÇ∞ ‚Üí Top N Îã®Ïñ¥ Ï∂îÏ∂ú ==========\n",
    "all_docs = []\n",
    "for docs in period_docs.values():\n",
    "    all_docs.extend(docs)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_all = vectorizer.fit_transform(all_docs)\n",
    "df_all = pd.DataFrame(X_all.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "top_words = df_all.sum().sort_values(ascending=False).head(TOP_N).index.tolist()\n",
    "\n",
    "# ========== ÏãúÍ∏∞Î≥Ñ mean TF-IDF Í≥ÑÏÇ∞ ==========\n",
    "period_mean = {}\n",
    "for period, docs in period_docs.items():\n",
    "    X = vectorizer.transform(docs)\n",
    "    df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # ÎàÑÎùΩ Îã®Ïñ¥ Î≥¥Ï†ï\n",
    "    for word in top_words:\n",
    "        if word not in df_tfidf.columns:\n",
    "            df_tfidf[word] = 0.0\n",
    "\n",
    "    # ÌèâÍ∑† TF-IDF\n",
    "    mean_tfidf = df_tfidf[top_words].mean(axis=0)\n",
    "    period_mean[period] = mean_tfidf\n",
    "\n",
    "# ========== Í≤∞Í≥º Ï†ïÎ¶¨ ==========\n",
    "mean_tfidf_df = pd.DataFrame(period_mean).T[top_words]  # Period x Top 20 Îã®Ïñ¥\n",
    "\n",
    "# ========== Z-score Í≥ÑÏÇ∞ ==========\n",
    "mean_val = mean_tfidf_df.values.mean()\n",
    "std_val = mean_tfidf_df.values.std()\n",
    "z_score_df = (mean_tfidf_df - mean_val) / std_val\n",
    "\n",
    "\n",
    "# ====== Ìè∞Ìä∏ ÏÑ§Ï†ï (Windows Í∏∞Ï§Ä) ======\n",
    "plt.rcParams[\"font.family\"] = \"Malgun Gothic\"  # Windows\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# ========== 2. ÌûàÌä∏Îßµ ==========\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(z_score_df, annot=True, fmt=\".4f\", cmap=\"YlGnBu\")\n",
    "\n",
    "plt.title(\"ÏãúÍ∏∞Î≥Ñ Top 20 Îã®Ïñ¥Ïùò ÌèâÍ∑† TF-IDF z-score ÌûàÌä∏Îßµ\")\n",
    "plt.xlabel(\"Îã®Ïñ¥\")\n",
    "plt.ylabel(\"ÏãúÍ∏∞\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56eb5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf Ï†ÑÏ≤¥ Îã®Ïñ¥ Ïàò\n",
    "\n",
    "#word frequency\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from kiwipiepy import Kiwi\n",
    "from collections import Counter\n",
    "\n",
    "# Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "base_dir = r\"C:\\Users\\ÎÖºÎ¨∏\"\n",
    "input_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
    "df = pd.read_excel(input_path, sheet_name=sheet_name)\n",
    "\n",
    "# ÏãúÍ∏∞ Íµ¨Î∂Ñ\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970‚Äì1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980‚Äì1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988‚Äì1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996‚Äì2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008‚Äì2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015‚Äì2023\"\n",
    "    else: return \"Unknown\"\n",
    "\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# ÌòïÌÉúÏÜå Î∂ÑÏÑùÍ∏∞\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# Î∂ÑÏÑùÌï† ÌíàÏÇ¨\n",
    "target_pos = [\"NNG\", \"VV\", \"VA\", \"MAG\"]\n",
    "\n",
    "def analyze_tokens(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    counters = {\n",
    "        \"NNG\": Counter(),\n",
    "        \"VV\": Counter(),\n",
    "        \"VA\": Counter(),\n",
    "        \"MAG\": Counter(),\n",
    "        \"COMBINED\": Counter()\n",
    "    }\n",
    "    for token in tokens:\n",
    "        word = token.form\n",
    "        tag = token.tag\n",
    "        if tag == \"VV\":\n",
    "            word += \"Îã§\"\n",
    "        if tag == \"VA\":\n",
    "            word += \"Îã§\"\n",
    "        if tag in target_pos:\n",
    "            counters[tag][word] += 1\n",
    "            counters[\"COMBINED\"][word] += 1\n",
    "    return counters\n",
    "\n",
    "# === Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ Î∂ÑÏÑù ===\n",
    "full_text = \" \".join(df[\"Î≥∏Î¨∏ÎÇ¥Ïö©\"].astype(str).tolist())\n",
    "total_counts = analyze_tokens(full_text)\n",
    "\n",
    "# Í≤∞Í≥º Ï†ÄÏû•\n",
    "top_combined = total_counts[\"COMBINED\"].most_common(20)\n",
    "top_nouns = total_counts[\"NNG\"].most_common(10)\n",
    "top_verbs = total_counts[\"VV\"].most_common(10)\n",
    "\n",
    "# === Ïã†Î¨∏ÏÇ¨Î≥Ñ / ÏãúÍ∏∞Î≥Ñ ===\n",
    "def grouped_word_counts(df, by=\"Ïã†Î¨∏ÏÇ¨Î™Ö\"):\n",
    "    result = {}\n",
    "    for key, group in df.groupby(by):\n",
    "        text = \" \".join(group[\"Î≥∏Î¨∏ÎÇ¥Ïö©\"].astype(str).tolist())\n",
    "        counts = analyze_tokens(text)\n",
    "        result[key] = {\n",
    "            \"COMBINED\": counts[\"COMBINED\"].most_common(10),\n",
    "            \"NNG\": counts[\"NNG\"].most_common(5),\n",
    "            \"VV\": counts[\"VV\"].most_common(5)\n",
    "        }\n",
    "    return result\n",
    "\n",
    "newspaper_results = grouped_word_counts(df, by=\"Ïã†Î¨∏ÏÇ¨Î™Ö\")\n",
    "period_results = grouped_word_counts(df, by=\"Period\")\n",
    "\n",
    "# ÌíàÏÇ¨Î≥Ñ Ï†ÑÏ≤¥ Îã®Ïñ¥ Ïàò Ï∂úÎ†•\n",
    "total_noun_count = sum(total_counts[\"NNG\"].values())\n",
    "total_verb_count = sum(total_counts[\"VV\"].values())\n",
    "total_adj_count = sum(total_counts[\"VA\"].values())\n",
    "total_adv_count = sum(total_counts[\"MAG\"].values())\n",
    "total_combined_count = sum(total_counts[\"COMBINED\"].values())\n",
    "\n",
    "print(\"\\n‚úÖ Ï†ÑÏ≤¥ Îã®Ïñ¥ Ïàò (ÎπàÎèÑ Í∏∞Ï§Ä):\")\n",
    "print(f\"  - Ï†ÑÏ≤¥ Ìï©Í≥Ñ: {total_combined_count}\")\n",
    "print(f\"  - Î™ÖÏÇ¨(NNG): {total_noun_count}\")\n",
    "print(f\"  - ÎèôÏÇ¨(VV): {total_verb_count}\")\n",
    "print(f\"  - ÌòïÏö©ÏÇ¨(VA): {total_adj_count}\")\n",
    "print(f\"  - Î∂ÄÏÇ¨(MAG): {total_adv_count}\")\n",
    "\n",
    "\n",
    "# Ï∂úÎ†• ÏòàÏãú\n",
    "print(\"‚úÖ Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ Top 20:\")\n",
    "print(top_combined)\n",
    "\n",
    "print(\"\\n‚úÖ Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ Top 10 Î™ÖÏÇ¨:\")\n",
    "print(top_nouns)\n",
    "\n",
    "print(\"\\n‚úÖ Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ Top 10 ÎèôÏÇ¨:\")\n",
    "print(top_verbs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf1201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# ===== ÏÇ¨Ïö©Ïûê Í≤ΩÎ°ú ÏÑ§Ï†ï =====\n",
    "base_dir = r\"C:\\Users\\ÎÖºÎ¨∏\"\n",
    "input_file = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text_Í∏∞ÏÇ¨\"\n",
    "exclude_words = {\"ÎØ∏ÌòºÎ™®\", \"ÌïòÎã§\"}\n",
    "\n",
    "# ===== Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞ =====\n",
    "df = pd.read_excel(input_file, sheet_name=sheet_name)\n",
    "\n",
    "# ===== ÌòïÌÉúÏÜå Î∂ÑÏÑù Î∞è Ï†ÑÏ≤òÎ¶¨ =====\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    words = []\n",
    "    for token in tokens:\n",
    "        if token.tag.startswith(\"NNG\"):\n",
    "            words.append(token.form)\n",
    "        elif token.tag.startswith(\"VV\"):\n",
    "            words.append(token.form + \"Îã§\")\n",
    "    return [w for w in words if w not in exclude_words]\n",
    "\n",
    "df[\"tokens\"] = df[\"Î≥∏Î¨∏ÎÇ¥Ïö©\"].astype(str).apply(lambda x: \" \".join(tokenize(x)))\n",
    "\n",
    "# ===== TF-IDF Í≥ÑÏÇ∞ =====\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"tokens\"])\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out(), index=df.index)\n",
    "\n",
    "# ===== TF-IDF Ìï©ÏÇ∞ (Îã®Ïñ¥Î≥Ñ) =====\n",
    "word_sum_tfidf = tfidf_df.sum(axis=0).to_frame(name=\"sumTF-IDF\")\n",
    "\n",
    "# ===== Í∏∞Ïà†ÌÜµÍ≥Ñ ÏöîÏïΩ =====\n",
    "word_sum_stats = word_sum_tfidf[\"sumTF-IDF\"].describe(percentiles=[.25, .5, .75]).round(3)\n",
    "stats_df = pd.DataFrame(word_sum_stats).T\n",
    "stats_df.index = [\"sumTF-IDF\"]\n",
    "stats_df = stats_df[[\"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"]]\n",
    "\n",
    "# ===== Í≤∞Í≥º Ï∂úÎ†• =====\n",
    "print(\"üìä Îã®Ïñ¥Î≥Ñ TF-IDF Ï¥ùÌï© ÌÜµÍ≥Ñ:\")\n",
    "print(stats_df)\n",
    "\n",
    "# ===== ÏÑ†ÌÉù: LaTeX ÌÖåÏù¥Î∏îÎ°ú Ï†ÄÏû• =====\n",
    "latex_output_path = os.path.join(base_dir, \"TFIDF_sum_stats_by_word.tex\")\n",
    "with open(latex_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(stats_df.to_latex(index=True, caption=\"Îã®Ïñ¥Î≥Ñ TF-IDF Ï¥ùÌï© ÌÜµÍ≥Ñ\", label=\"tab:word_tfidf_stats\"))\n",
    "\n",
    "print(f\"\\nüìÑ LaTeX ÌÖåÏù¥Î∏î Ï†ÄÏû• ÏôÑÎ£å: {latex_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518ab82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Îã®Ïñ¥Î≥Ñ sumTF-IDF Í≥ÑÏÇ∞ =====\n",
    "word_sum_tfidf = tfidf_df.sum(axis=0).to_frame(name=\"sumTF-IDF\")\n",
    "\n",
    "# ===== Ïà´Ïûê Ìè¨Îß∑ÏùÑ ÏúÑÌïú Î≥µÏÇ¨Î≥∏ ÏÉùÏÑ± =====\n",
    "word_sum_tfidf_fmt = word_sum_tfidf.copy()\n",
    "word_sum_tfidf_fmt[\"sumTF-IDF\"] = word_sum_tfidf_fmt[\"sumTF-IDF\"].apply(lambda x: f\"{x:,.0f}\")\n",
    "\n",
    "# ===== ÏÉÅÏúÑ 10Í∞ú Îã®Ïñ¥ =====\n",
    "top_10 = word_sum_tfidf_fmt.sort_values(by=\"sumTF-IDF\", ascending=False).head(10)\n",
    "top_10.index.name = \"Top 10 (High TF-IDF)\"\n",
    "\n",
    "# ===== ÌïòÏúÑ 10Í∞ú Îã®Ïñ¥ =====\n",
    "bottom_10 = word_sum_tfidf_fmt.sort_values(by=\"sumTF-IDF\", ascending=True).head(10)\n",
    "bottom_10.index.name = \"Top 10 (Low TF-IDF)\"\n",
    "\n",
    "# ===== Ï∂úÎ†• =====\n",
    "print(\"üîù TF-IDF ÏÉÅÏúÑ 10Í∞ú Îã®Ïñ¥:\")\n",
    "print(top_10)\n",
    "print(\"\\nüîª TF-IDF ÌïòÏúÑ 10Í∞ú Îã®Ïñ¥:\")\n",
    "print(bottom_10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff403c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
