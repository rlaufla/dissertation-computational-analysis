{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e58535",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kiwipiepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "\n",
    "kiwi = Kiwi()\n",
    "print(kiwi.analyze(\"정말 기분이 좋다\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd859f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#기존코드_successful\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from kiwipiepy import Kiwi\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# 한글 폰트 설정\n",
    "matplotlib.rc('font', family='Malgun Gothic')\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ========== 사용자 설정 ==========\n",
    "base_dir = r\"C:\\Users\\논문\"\n",
    "input_excel_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "senti_path = os.path.join(base_dir, \"for the analysing\", \"data\", \"SentiWord_info.json\")\n",
    "output_excel_path = os.path.join(base_dir, \"3\", \"result.xlsx\")\n",
    "# ================================\n",
    "\n",
    "# 1. 감성 사전 로드\n",
    "with open(senti_path, encoding='utf-8-sig') as f:\n",
    "    senti_dict = json.load(f)\n",
    "senti_lookup = {item['word']: int(item['polarity']) for item in senti_dict}\n",
    "\n",
    "# 2. 데이터 불러오기\n",
    "df = pd.read_excel(input_excel_path, sheet_name=sheet_name)\n",
    "\n",
    "# 3. periodisation 생성\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979:\n",
    "        return \"1. 1970–1979\"\n",
    "    elif 1980 <= year <= 1987:\n",
    "        return \"2. 1980–1987\"\n",
    "    elif 1988 <= year <= 1995:\n",
    "        return \"3. 1988–1995\"\n",
    "    elif 1996 <= year <= 2007:\n",
    "        return \"4. 1996–2007\"\n",
    "    elif 2008 <= year <= 2014:\n",
    "        return \"5. 2008–2014\"\n",
    "    elif 2015 <= year <= 2023:\n",
    "        return \"6. 2015–2023\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# 4. KIWI 초기화\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# 5. 기사별 감성 분석\n",
    "records = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = str(row[\"본문내용\"])\n",
    "    period = row[\"Period\"]\n",
    "\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    total_tokens = 0\n",
    "    senti_scores = []\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        word = token.form\n",
    "        tag = token.tag\n",
    "        total_tokens += 1\n",
    "        if tag.startswith(\"VV\") or tag.startswith(\"VA\"):\n",
    "            word += \"다\"\n",
    "        if word in senti_lookup:\n",
    "            score = senti_lookup[word]\n",
    "            senti_scores.append(score)\n",
    "            if score > 0:\n",
    "                pos_count += 1\n",
    "            elif score < 0:\n",
    "                neg_count += 1\n",
    "\n",
    "    senti_count = len(senti_scores)\n",
    "    avg_score = round(np.mean(senti_scores), 3) if senti_scores else 0.0\n",
    "    ratio = round((senti_count / total_tokens * 100), 2) if total_tokens else 0.0\n",
    "\n",
    "    records.append({\n",
    "        \"분류\": period,\n",
    "        \"전체단어수\": total_tokens,\n",
    "        \"감성단어수\": senti_count,\n",
    "        \"긍정단어수\": pos_count,\n",
    "        \"부정단어수\": neg_count,\n",
    "        \"평균감성점수\": avg_score,\n",
    "        \"감성단어비율\": ratio\n",
    "    })\n",
    "\n",
    "# 6. DataFrame으로 저장\n",
    "result_df = pd.DataFrame(records)\n",
    "\n",
    "# 7. 시기별 통계 요약\n",
    "summary = result_df.groupby(\"분류\").agg({\n",
    "    \"전체단어수\": \"sum\",\n",
    "    \"감성단어수\": \"sum\",\n",
    "    \"긍정단어수\": \"sum\",\n",
    "    \"부정단어수\": \"sum\",\n",
    "    \"평균감성점수\": \"mean\",\n",
    "    \"감성단어비율\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "# 8. 저장\n",
    "summary.to_excel(output_excel_path, index=False)\n",
    "print(f\"✅ 분석 완료! 결과 저장 위치: {output_excel_path}\")\n",
    "\n",
    "#밑에 코드 말고 이걸로 할거임. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8493b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#수정코드_형태소분석 단어 더 많이 감성분석할수 있게\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from kiwipiepy import Kiwi\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# 한글 폰트 설정\n",
    "matplotlib.rc('font', family='Malgun Gothic')\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ========== 사용자 설정 ==========\n",
    "base_dir = r\"C:\\Users\\논문\"\n",
    "input_excel_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "senti_path = os.path.join(base_dir, \"for the analysing\", \"data\", \"SentiWord_info.json\")\n",
    "output_excel_path = os.path.join(base_dir, \"3\", \"result.xlsx\")\n",
    "# ================================\n",
    "\n",
    "# 1. 감성 사전 로드\n",
    "with open(senti_path, encoding='utf-8-sig') as f:\n",
    "    senti_dict = json.load(f)\n",
    "senti_lookup = {item['word']: int(item['polarity']) for item in senti_dict}\n",
    "\n",
    "# 2. 데이터 불러오기\n",
    "df = pd.read_excel(input_excel_path, sheet_name=sheet_name)\n",
    "\n",
    "# 3. periodisation 생성\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979:\n",
    "        return \"1. 1970–1979\"\n",
    "    elif 1980 <= year <= 1987:\n",
    "        return \"2. 1980–1987\"\n",
    "    elif 1988 <= year <= 1995:\n",
    "        return \"3. 1988–1995\"\n",
    "    elif 1996 <= year <= 2007:\n",
    "        return \"4. 1996–2007\"\n",
    "    elif 2008 <= year <= 2014:\n",
    "        return \"5. 2008–2014\"\n",
    "    elif 2015 <= year <= 2023:\n",
    "        return \"6. 2015–2023\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# 4. KIWI 초기화\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# 5. 감성어 품사 정의 (동사, 형용사, 명사, 어근, 부사)\n",
    "senti_pos_tags = [\"VV\", \"VA\", \"NNG\", \"XR\", \"MAG\"]\n",
    "\n",
    "# 6. 기사별 감성 분석\n",
    "records = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = str(row[\"본문내용\"])\n",
    "    period = row[\"Period\"]\n",
    "\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    total_tokens = 0\n",
    "    senti_scores = []\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        word = token.form\n",
    "        tag = token.tag\n",
    "        total_tokens += 1\n",
    "\n",
    "        # 동사/형용사는 어간 + 다 형태로 복원\n",
    "        if tag.startswith(\"VV\") or tag.startswith(\"VA\"):\n",
    "            word += \"다\"\n",
    "\n",
    "        # 감성사전에 포함될 품사만 매칭 시도\n",
    "        if tag[:3] in senti_pos_tags or tag in senti_pos_tags:\n",
    "            if word in senti_lookup:\n",
    "                score = senti_lookup[word]\n",
    "                senti_scores.append(score)\n",
    "                if score > 0:\n",
    "                    pos_count += 1\n",
    "                elif score < 0:\n",
    "                    neg_count += 1\n",
    "\n",
    "    senti_count = len(senti_scores)\n",
    "    avg_score = round(np.mean(senti_scores), 3) if senti_scores else 0.0\n",
    "    ratio = round((senti_count / total_tokens * 100), 2) if total_tokens else 0.0\n",
    "\n",
    "    records.append({\n",
    "        \"분류\": period,\n",
    "        \"전체단어수\": total_tokens,\n",
    "        \"감성단어수\": senti_count,\n",
    "        \"긍정단어수\": pos_count,\n",
    "        \"부정단어수\": neg_count,\n",
    "        \"평균감성점수\": avg_score,\n",
    "        \"감성단어비율\": ratio\n",
    "    })\n",
    "\n",
    "# 7. DataFrame으로 저장\n",
    "result_df = pd.DataFrame(records)\n",
    "\n",
    "# 8. 시기별 통계 요약\n",
    "summary = result_df.groupby(\"분류\").agg({\n",
    "    \"전체단어수\": \"sum\",\n",
    "    \"감성단어수\": \"sum\",\n",
    "    \"긍정단어수\": \"sum\",\n",
    "    \"부정단어수\": \"sum\",\n",
    "    \"평균감성점수\": \"mean\",\n",
    "    \"감성단어비율\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "# 9. 저장\n",
    "summary.to_excel(output_excel_path, index=False)\n",
    "print(f\"✅ 분석 완료! 결과 저장 위치: {output_excel_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e8e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#전체 텍스트 감성분석결과\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# 사용자 설정\n",
    "base_dir = r\"C:\\Users\\논문\"\n",
    "input_excel_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "senti_path = os.path.join(base_dir, \"for the analysing\", \"data\", \"SentiWord_info.json\")\n",
    "\n",
    "# 감성사전 불러오기\n",
    "with open(senti_path, encoding='utf-8-sig') as f:\n",
    "    senti_dict = json.load(f)\n",
    "senti_lookup = {item['word']: int(item['polarity']) for item in senti_dict}\n",
    "\n",
    "# 텍스트 불러오기\n",
    "df = pd.read_excel(input_excel_path, sheet_name=sheet_name)\n",
    "all_text = \" \".join(df[\"본문내용\"].astype(str).tolist())  # 전체 기사 합치기\n",
    "\n",
    "# 형태소 분석기 초기화\n",
    "kiwi = Kiwi()\n",
    "tokens = kiwi.tokenize(all_text)\n",
    "\n",
    "# 감성 품사: VV(동사), VA(형용사), NNG(명사), XR(어근), MAG(부사)\n",
    "senti_pos_tags = [\"VV\", \"VA\", \"NNG\", \"XR\", \"MAG\"]\n",
    "\n",
    "# 감성 분석\n",
    "total_tokens = 0\n",
    "senti_scores = []\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "\n",
    "for token in tokens:\n",
    "    word = token.form\n",
    "    tag = token.tag\n",
    "    total_tokens += 1\n",
    "\n",
    "    # 동사/형용사: 원형 복원\n",
    "    if tag.startswith(\"VV\") or tag.startswith(\"VA\"):\n",
    "        word += \"다\"\n",
    "\n",
    "    # 감성사전 매칭\n",
    "    if tag[:3] in senti_pos_tags or tag in senti_pos_tags:\n",
    "        if word in senti_lookup:\n",
    "            score = senti_lookup[word]\n",
    "            senti_scores.append(score)\n",
    "            if score > 0:\n",
    "                pos_count += 1\n",
    "            elif score < 0:\n",
    "                neg_count += 1\n",
    "\n",
    "# 결과 계산\n",
    "senti_count = len(senti_scores)\n",
    "avg_score = round(np.mean(senti_scores), 3) if senti_scores else 0.0\n",
    "ratio = round((senti_count / total_tokens * 100), 2) if total_tokens else 0.0\n",
    "\n",
    "# 출력\n",
    "print(\"📊 전체 텍스트 감성 분석 결과:\")\n",
    "print(f\"전체단어수       : {total_tokens}\")\n",
    "print(f\"감성단어수       : {senti_count}\")\n",
    "print(f\"긍정단어수       : {pos_count}\")\n",
    "print(f\"부정단어수       : {neg_count}\")\n",
    "print(f\"평균감성점수     : {avg_score}\")\n",
    "print(f\"감성단어비율 (%) : {ratio}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c078236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word frequency\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from kiwipiepy import Kiwi\n",
    "from collections import Counter\n",
    "\n",
    "# 경로 설정\n",
    "base_dir = r\"C:\\Users\\논문\"\n",
    "input_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "\n",
    "# 데이터 불러오기\n",
    "df = pd.read_excel(input_path, sheet_name=sheet_name)\n",
    "\n",
    "# 시기 구분\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970–1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980–1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988–1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996–2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008–2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015–2023\"\n",
    "    else: return \"Unknown\"\n",
    "\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# 형태소 분석기\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# 분석할 품사\n",
    "target_pos = [\"NNG\", \"VV\", \"VA\", \"MAG\"]\n",
    "\n",
    "def analyze_tokens(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    counters = {\n",
    "        \"NNG\": Counter(),\n",
    "        \"VV\": Counter(),\n",
    "        \"VA\": Counter(),\n",
    "        \"MAG\": Counter(),\n",
    "        \"COMBINED\": Counter()\n",
    "    }\n",
    "    for token in tokens:\n",
    "        word = token.form\n",
    "        tag = token.tag\n",
    "        if tag == \"VV\":\n",
    "            word += \"다\"\n",
    "        if tag == \"VA\":\n",
    "            word += \"다\"\n",
    "        if tag in target_pos:\n",
    "            counters[tag][word] += 1\n",
    "            counters[\"COMBINED\"][word] += 1\n",
    "    return counters\n",
    "\n",
    "# === 전체 텍스트 분석 ===\n",
    "full_text = \" \".join(df[\"본문내용\"].astype(str).tolist())\n",
    "total_counts = analyze_tokens(full_text)\n",
    "\n",
    "# 결과 저장\n",
    "top_combined = total_counts[\"COMBINED\"].most_common(20)\n",
    "top_nouns = total_counts[\"NNG\"].most_common(10)\n",
    "top_verbs = total_counts[\"VV\"].most_common(10)\n",
    "\n",
    "# === 신문사별 / 시기별 ===\n",
    "def grouped_word_counts(df, by=\"신문사명\"):\n",
    "    result = {}\n",
    "    for key, group in df.groupby(by):\n",
    "        text = \" \".join(group[\"본문내용\"].astype(str).tolist())\n",
    "        counts = analyze_tokens(text)\n",
    "        result[key] = {\n",
    "            \"COMBINED\": counts[\"COMBINED\"].most_common(10),\n",
    "            \"NNG\": counts[\"NNG\"].most_common(5),\n",
    "            \"VV\": counts[\"VV\"].most_common(5)\n",
    "        }\n",
    "    return result\n",
    "\n",
    "period_results = grouped_word_counts(df, by=\"Period\")\n",
    "\n",
    "# 출력 예시\n",
    "print(\"✅ 전체 텍스트 Top 20:\")\n",
    "print(top_combined)\n",
    "\n",
    "print(\"\\n✅ 전체 텍스트 Top 10 명사:\")\n",
    "print(top_nouns)\n",
    "\n",
    "print(\"\\n✅ 전체 텍스트 Top 10 동사:\")\n",
    "print(top_verbs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c26352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#히트맵, Z-score\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from kiwipiepy import Kiwi\n",
    "from collections import Counter\n",
    "from scipy.stats import zscore\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = 'Malgun Gothic'  # 윈도우 기본 한글 글꼴\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False     # 마이너스 기호 깨짐 방지\n",
    "\n",
    "\n",
    "# -------------------- 1. 파일 불러오기 --------------------\n",
    "base_dir = r\"C:\\Users\\논문\"\n",
    "input_file = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "\n",
    "df = pd.read_excel(input_file, sheet_name=sheet_name)\n",
    "\n",
    "# -------------------- 2. 시기 컬럼 만들기 --------------------\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970–1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980–1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988–1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996–2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008–2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015–2023\"\n",
    "    else: return \"Unknown\"\n",
    "\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# -------------------- 3. 형태소 분석기 --------------------\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# -------------------- 4. 시기별 단어 빈도 수집 --------------------\n",
    "period_word_counts = {}\n",
    "all_top_words = set()\n",
    "\n",
    "for period, group in df.groupby(\"Period\"):\n",
    "    text = \" \".join(group[\"본문내용\"].astype(str).tolist())\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    counter = Counter()\n",
    "\n",
    "    for token in tokens:\n",
    "        word = token.form\n",
    "        tag = token.tag\n",
    "        if tag.startswith(\"VV\"):  # 동사\n",
    "            word += \"다\"\n",
    "            counter[word] += 1\n",
    "        elif tag.startswith(\"NNG\"):  # 명사\n",
    "            counter[word] += 1\n",
    "\n",
    "    top_words = counter.most_common(15)\n",
    "    all_top_words.update([word for word, _ in top_words])\n",
    "    period_word_counts[period] = counter\n",
    "\n",
    "# -------------------- 5. 빈도수 행렬 생성 --------------------\n",
    "top_words_list = sorted(all_top_words)\n",
    "periods = sorted(period_word_counts.keys())\n",
    "freq_matrix = pd.DataFrame(index=periods, columns=top_words_list).fillna(0)\n",
    "\n",
    "for period in periods:\n",
    "    counter = period_word_counts[period]\n",
    "    for word in top_words_list:\n",
    "        freq_matrix.loc[period, word] = counter[word]\n",
    "\n",
    "# -------------------- 6. Z-score 정규화 --------------------\n",
    "z_freq_matrix = freq_matrix.apply(zscore, axis=0)\n",
    "\n",
    "# -------------------- 7. 히트맵 시각화 및 저장 --------------------\n",
    "# Z-score 히트맵\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.heatmap(z_freq_matrix, cmap=\"coolwarm\", center=0, annot=True, fmt=\".2f\")\n",
    "plt.title(\"시기별 단어 사용 (z-score 정규화)\", fontsize=14)\n",
    "plt.xlabel(\"단어\", fontsize=12)\n",
    "plt.ylabel(\"시기\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(base_dir, \"3\", \"zscore_heatmap.png\"), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 원본 빈도 히트맵\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.heatmap(freq_matrix.astype(int), cmap=\"YlGnBu\", annot=True, fmt=\"d\")\n",
    "plt.title(\"시기별 상위 단어 사용 히트맵 (명사 + 동사)\", fontsize=14)\n",
    "plt.xlabel(\"단어\", fontsize=12)\n",
    "plt.ylabel(\"시기\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(base_dir, \"3\", \"freq_heatmap.png\"), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0228ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from kiwipiepy import Kiwi\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ------------------- 1. 데이터 경로 -------------------\n",
    "base_dir = r\"C:\\Users\\논문\"\n",
    "input_file = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "\n",
    "df = pd.read_excel(input_file, sheet_name=sheet_name)\n",
    "\n",
    "# ------------------- 2. 시기 매핑 -------------------\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970–1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980–1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988–1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996–2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008–2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015–2023\"\n",
    "    else: return \"Unknown\"\n",
    "\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# ------------------- 3. 시기별 전체 문서 생성 -------------------\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def extract_tokens(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        tag = token.tag\n",
    "        word = token.form\n",
    "        if tag.startswith(\"VV\"):  # 동사\n",
    "            word += \"다\"\n",
    "            result.append(word)\n",
    "        elif tag.startswith(\"NNG\"):  # 명사\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "# 시기별 텍스트 리스트 생성\n",
    "period_docs = df.groupby(\"Period\")[\"본문내용\"].apply(lambda texts: \" \".join(texts.astype(str))).to_dict()\n",
    "\n",
    "# 시기별 문서에서 명사+동사 토큰 추출 후 다시 문자열로 변환\n",
    "period_processed_docs = {}\n",
    "for period, text in period_docs.items():\n",
    "    tokens = extract_tokens(text)\n",
    "    period_processed_docs[period] = \" \".join(tokens)\n",
    "\n",
    "# ------------------- 4. TF-IDF 계산 -------------------\n",
    "vectorizer = TfidfVectorizer()\n",
    "corpus = [period_processed_docs[period] for period in sorted(period_processed_docs.keys())]\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# ------------------- 5. 결과 정리 -------------------\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(),\n",
    "                        index=sorted(period_processed_docs.keys()),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# 상위 TF-IDF 단어 15개 기준 단어 리스트 만들기\n",
    "top_words = set()\n",
    "for period in tfidf_df.index:\n",
    "    top = tfidf_df.loc[period].sort_values(ascending=False).head(15).index.tolist()\n",
    "    top_words.update(top)\n",
    "\n",
    "top_words = sorted(top_words)\n",
    "filtered_tfidf_df = tfidf_df[top_words]\n",
    "\n",
    "# ------------------- 6. 히트맵 시각화 및 저장 -------------------\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.heatmap(filtered_tfidf_df, annot=True, cmap=\"YlGnBu\", fmt=\".2f\")\n",
    "plt.title(\"시기별 상위 TF-IDF 단어 히트맵 (명사 + 동사)\", fontsize=14)\n",
    "plt.xlabel(\"단어\", fontsize=12)\n",
    "plt.ylabel(\"시기\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join(base_dir, \"3\", \"tfidf_heatmap.png\"), dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db17da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#하다/미혼모 뺀 TF- IDF, 기간별\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# 경로 설정\n",
    "base_dir = r\"C:\\Users\\논문\"\n",
    "input_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "\n",
    "# 데이터 불러오기\n",
    "df = pd.read_excel(input_path, sheet_name=sheet_name)\n",
    "\n",
    "# 시기 정의\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970–1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980–1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988–1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996–2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008–2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015–2023\"\n",
    "    else: return \"Unknown\"\n",
    "\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "\n",
    "# 형태소 분석기\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# 전처리 + 제외할 단어\n",
    "EXCLUDE_WORDS = {\"미혼모\", \"하다\"}\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    return [\n",
    "        (token.form + \"다\" if token.tag.startswith(\"VV\") else token.form)\n",
    "        for token in tokens\n",
    "        if token.tag.startswith((\"NNG\", \"VV\")) and\n",
    "           ((token.form + \"다\") if token.tag.startswith(\"VV\") else token.form) not in EXCLUDE_WORDS\n",
    "    ]\n",
    "\n",
    "# TF-IDF 계산 함수\n",
    "def compute_tfidf(grouped_texts, top_n=30):\n",
    "    corpus = grouped_texts.apply(lambda x: \" \".join(tokenize_text(\" \".join(x.astype(str)))))\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    tfidf_df = pd.DataFrame(X.toarray(), index=grouped_texts.index, columns=vectorizer.get_feature_names_out())\n",
    "    top_words = tfidf_df.max().sort_values(ascending=False).head(top_n).index.tolist()\n",
    "    return tfidf_df[top_words]\n",
    "\n",
    "# ==================== 시기별 TF-IDF ====================\n",
    "period_tfidf = compute_tfidf(df.groupby(\"Period\")[\"본문내용\"])\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(period_tfidf.T, cmap=\"YlGnBu\", annot=True, fmt=\".2f\", linewidths=0.5, cbar_kws={'label': 'TF-IDF'})\n",
    "plt.title(\"시기별 TF-IDF 상위 단어 비교 (미혼모, 하다 제외)\", fontsize=14)\n",
    "plt.xlabel(\"시기\", fontsize=12)\n",
    "plt.ylabel(\"단어\", fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(base_dir, \"3\", \"시기별_TFIDF_히트맵_제외단어제거.png\"))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dbffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#위에것은 kiwi형태소 분석 안했기 때문에 다시 형태소 분석 후 TF 분석, 시기별로 나눈 뒤 각 시기마다 TF-IDF를 새로 계산한 것\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# 파일 경로\n",
    "base_dir = r\"C:\\Users\\논문\"\n",
    "file_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "\n",
    "# 데이터 불러오기\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "# 시기 분류\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970–1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980–1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988–1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996–2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008–2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015–2023\"\n",
    "    else: return \"Unknown\"\n",
    "\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period) #연도(Year) 정보를 바탕으로 6개의 시기로 분류합니다.\n",
    "\n",
    "\n",
    "# 형태소 분석기\n",
    "kiwi = Kiwi()\n",
    "EXCLUDE_WORDS = {\"미혼모\", \"하다\"}\n",
    "\n",
    "# 텍스트 토큰화\n",
    "def tokenize_text(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    return [\n",
    "        (token.form + \"다\" if token.tag.startswith(\"VV\") else token.form)\n",
    "        for token in tokens\n",
    "        if token.tag.startswith((\"NNG\", \"VV\")) and\n",
    "           ((token.form + \"다\") if token.tag.startswith(\"VV\") else token.form) not in EXCLUDE_WORDS\n",
    "    ]\n",
    "\n",
    "# 명사(NNG), 동사(VV)만 추출하고, 불용어(예: ‘미혼모’, ‘하다’) 제외.동사는 기본형 + “다” 형태로 만듦 (e.g., \"보\" → \"보다\")\n",
    "\n",
    "\n",
    "\n",
    "# TF-IDF 계산\n",
    "def compute_tfidf(grouped_df, top_n=30):\n",
    "    labels = []\n",
    "    docs = []\n",
    "    for label, group in grouped_df:\n",
    "        full_text = \" \".join(group[\"본문내용\"].astype(str))\n",
    "        tokens = tokenize_text(full_text)\n",
    "        docs.append(\" \".join(tokens)) #각 시기별로 기사들을 모아 하나의 텍스트로 만들고 TF-IDF 계산\n",
    "        labels.append(label)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    tfidf_df = pd.DataFrame(X.toarray(), index=labels, columns=vectorizer.get_feature_names_out())\n",
    "    top_words = tfidf_df.max().sort_values(ascending=False).head(top_n).index.tolist()\n",
    "    return tfidf_df[top_words]\n",
    "\n",
    "# 실행\n",
    "period_tfidf = compute_tfidf(df.groupby(\"Period\"))\n",
    "\n",
    "\n",
    "# 저장\n",
    "period_tfidf.to_csv(os.path.join(base_dir, \"3\", \"형태소기반_시기별_TFIDF.csv\"), encoding=\"utf-8-sig\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6623fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#히트맵\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "\n",
    "# ----------- [1] 폰트 설정 (Windows / Mac / Linux 대응) -----------\n",
    "try:\n",
    "    matplotlib.rc(\"font\", family=\"Malgun Gothic\")  # Windows\n",
    "except:\n",
    "    try:\n",
    "        matplotlib.rc(\"font\", family=\"AppleGothic\")  # Mac\n",
    "    except:\n",
    "        matplotlib.rc(\"font\", family=\"DejaVu Sans\")  # Linux (대체)\n",
    "\n",
    "matplotlib.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# ----------- [2] 경로 설정 및 데이터 불러오기 -----------\n",
    "base_dir = r\"C:\\Users\\논문\\3\"\n",
    "period_path = os.path.join(base_dir, \"형태소기반_시기별_TFIDF.csv\")\n",
    "\n",
    "\n",
    "period_tfidf = pd.read_csv(period_path, index_col=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_tfidf_heatmap(tfidf_df, title, figsize=(14, 8), output_file=None):\n",
    "    # 모든 값이 0인 열 제거\n",
    "    tfidf_df_clean = tfidf_df.loc[:, (tfidf_df != 0).any(axis=0)].dropna(axis=1, how='all')\n",
    "\n",
    "    # 모든 값이 0이거나 NaN이면 출력 건너뛰기\n",
    "    if tfidf_df_clean.shape[1] == 0:\n",
    "        print(f\"⚠️ 히트맵 생략됨: '{title}'에 표시할 유효한 TF-IDF 단어 없음.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(tfidf_df_clean.T, cmap=\"YlGnBu\", annot=True, fmt=\".2f\",\n",
    "                linewidths=0.5, cbar_kws={'label': 'TF-IDF'}, square=False)\n",
    "\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(\"분류\", fontsize=13)\n",
    "    plt.ylabel(\"단어\", fontsize=13)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_file:\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✅ 저장 완료: {output_file}\")\n",
    "    plt.show()\n",
    "\n",
    "# ----------- [4] 시각화 실행 -----------\n",
    "plot_tfidf_heatmap(period_tfidf, \"시기별 TF-IDF 상위 단어 히트맵\",\n",
    "                   output_file=os.path.join(base_dir, \"TFIDF_시기별_히트맵.png\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5befd08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kiwipiepy scikit-learn pandas matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26f0236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from kiwipiepy import Kiwi\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# ============ 사용자 설정 ============\n",
    "base_dir = r\"C:\\Users\\논문\"\n",
    "input_file = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "exclude_words = {\"미혼모\", \"하다\"}\n",
    "top_n = 15\n",
    "output_file_period = os.path.join(base_dir, \"TF_IDF123_시기별.png\")\n",
    "\n",
    "\n",
    "# ============ 한글 폰트 설정 ============\n",
    "font_path = \"C:/Windows/Fonts/malgun.ttf\"\n",
    "if os.path.exists(font_path):\n",
    "    fm.fontManager.addfont(font_path)\n",
    "    plt.rc(\"font\", family=\"Malgun Gothic\")\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# ============ 데이터 로드 ============\n",
    "df = pd.read_excel(input_file, sheet_name=sheet_name)\n",
    "\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970–1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980–1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988–1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996–2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008–2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015–2023\"\n",
    "    else: return \"Unknown\"\n",
    "\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "\n",
    "# ============ 형태소 분석 및 전처리 ============\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    words = []\n",
    "    for token in tokens:\n",
    "        tag = token.tag\n",
    "        word = token.form\n",
    "        if tag.startswith(\"NNG\"):\n",
    "            words.append(word)\n",
    "        elif tag.startswith(\"VV\"):\n",
    "            words.append(word + \"다\")\n",
    "    return [w for w in words if w not in exclude_words]\n",
    "\n",
    "df[\"tokens\"] = df[\"본문내용\"].astype(str).apply(lambda x: \" \".join(tokenize(x)))\n",
    "\n",
    "# ============ TF-IDF 계산 및 히트맵 ============\n",
    "def compute_tfidf_heatmap(df, group_col, title, output_path):\n",
    "    grouped = df.groupby(group_col)[\"tokens\"].apply(lambda x: \" \".join(x))\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(grouped)\n",
    "    tfidf_df = pd.DataFrame(X.toarray(), index=grouped.index, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    top_words = set()\n",
    "    for _, row in tfidf_df.iterrows():\n",
    "        top_words.update(row.sort_values(ascending=False).head(top_n).index)\n",
    "    tfidf_top = tfidf_df[list(top_words)]\n",
    "\n",
    "    if tfidf_top.empty or (tfidf_top.sum(axis=0) == 0).all():\n",
    "        print(f\"⚠️ 히트맵 생략됨: '{title}'에 표시할 유효한 TF-IDF 단어 없음.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(tfidf_top.T, cmap=\"YlGnBu\", annot=True, fmt=\".2f\",\n",
    "                linewidths=0.5, cbar_kws={\"label\": \"TF-IDF\"})\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(group_col, fontsize=12)\n",
    "    plt.ylabel(\"단어\", fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "# 실행\n",
    "compute_tfidf_heatmap(df, \"Period\", \"시기별 TF-IDF 상위 단어 히트맵\", output_file_period)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d30bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-Idf Unigram 유사도 꺾은선으로, 안 쓰는걸로. \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from kiwipiepy import Kiwi\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# ===== 사용자 설정 =====\n",
    "base_dir = r\"C:\\Users\\논문\"\n",
    "input_file = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "exclude_words = {\"미혼모\", \"하다\"}\n",
    "output_file = os.path.join(base_dir, \"TF_IDF123_그래프.png\")\n",
    "\n",
    "# ===== 한글 폰트 설정 =====\n",
    "font_path = \"C:/Windows/Fonts/malgun.ttf\"\n",
    "if os.path.exists(font_path):\n",
    "    fm.fontManager.addfont(font_path)\n",
    "    plt.rc(\"font\", family=\"Malgun Gothic\")\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# ===== 데이터 불러오기 =====\n",
    "df = pd.read_excel(input_file, sheet_name=sheet_name)\n",
    "\n",
    "# ===== 시기 정의 함수 =====\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970–1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980–1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988–1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996–2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008–2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015–2023\"\n",
    "    else: return \"Unknown\"\n",
    "\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# ===== 형태소 분석 =====\n",
    "kiwi = Kiwi()\n",
    "def tokenize(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    words = []\n",
    "    for token in tokens:\n",
    "        if token.tag.startswith(\"NNG\"):\n",
    "            words.append(token.form)\n",
    "        elif token.tag.startswith(\"VV\"):\n",
    "            words.append(token.form + \"다\")\n",
    "    return [w for w in words if w not in exclude_words]\n",
    "\n",
    "df[\"tokens\"] = df[\"본문내용\"].astype(str).apply(lambda x: \" \".join(tokenize(x)))\n",
    "\n",
    "# ===== 시기별 문서 묶기 =====\n",
    "period_grouped = df.groupby(\"Period\")[\"tokens\"].apply(lambda x: \" \".join(x)).sort_index()\n",
    "\n",
    "# ===== TF-IDF 벡터화 및 유사도 계산 =====\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(period_grouped)\n",
    "cos_sim = cosine_similarity(X)\n",
    "\n",
    "# ===== 유사도 DataFrame =====\n",
    "periods = period_grouped.index.tolist()\n",
    "sim_df = pd.DataFrame(cos_sim, index=periods, columns=periods)\n",
    "\n",
    "# ===== 꺾은선 그래프 =====\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, period in enumerate(periods):\n",
    "    if i != 0:\n",
    "        plt.plot(periods[:i+1], sim_df.iloc[i, :i+1], marker='o', label=f\"{period}\")\n",
    "\n",
    "plt.title(\"시기별 TF-IDF Unigram 유사도 비교\", fontsize=14)\n",
    "plt.ylabel(\"Cosine Similarity\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"기준 시기\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_file)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8360c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#axis = 1 로 만들어서 하나의 문서 안에 등장한 모든 단어들의 TF-IDF 가중치 합 을 구한거임. 잘못되서 밑에서 다시 구할ㄹㅕ고\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from kiwipiepy import Kiwi\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ===== 사용자 경로 설정 =====\n",
    "base_dir = r\"C:\\Users\\논문\"\n",
    "input_file = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "exclude_words = {\"미혼모\", \"하다\"}\n",
    "\n",
    "# ===== 데이터 불러오기 =====\n",
    "df = pd.read_excel(input_file, sheet_name=sheet_name)\n",
    "\n",
    "# ===== 형태소 분석 및 전처리 =====\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    words = []\n",
    "    for token in tokens:\n",
    "        if token.tag.startswith(\"NNG\"):\n",
    "            words.append(token.form)\n",
    "        elif token.tag.startswith(\"VV\"):\n",
    "            words.append(token.form + \"다\")\n",
    "    return [w for w in words if w not in exclude_words]\n",
    "\n",
    "df[\"tokens\"] = df[\"본문내용\"].astype(str).apply(lambda x: \" \".join(tokenize(x)))\n",
    "\n",
    "# ===== TF-IDF 계산 =====\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"tokens\"])\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# ===== TF-IDF 합산 (문서별) =====\n",
    "df[\"sum_tfidf\"] = tfidf_df.sum(axis=1)\n",
    "\n",
    "# ===== 기술통계 요약 =====\n",
    "stats = df[\"sum_tfidf\"].describe(percentiles=[.25, .5, .75]).round(3)\n",
    "stats_df = pd.DataFrame(stats).T\n",
    "stats_df.index = [\"sumTF-IDF\"]\n",
    "stats_df = stats_df[[\"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"]]\n",
    "\n",
    "# ===== 결과 출력 =====\n",
    "print(\"📊 TF-IDF 가중치 총합 기술통계:\")\n",
    "print(stats_df)\n",
    "\n",
    "# ===== 선택: LaTeX 테이블로 저장 =====\n",
    "latex_output_path = os.path.join(base_dir, \"TFIDF_sum_stats.tex\")\n",
    "with open(latex_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(stats_df.to_latex(index=True, caption=\"TF-IDF 가중치 총합 기술통계\", label=\"tab:tfidf_stats\"))\n",
    "\n",
    "print(f\"\\n📄 LaTeX 테이블 저장 완료: {latex_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa97b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#아까 구한 TF-IDF 엑셀로 표로 만들려고.\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from kiwipiepy import Kiwi\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# ============ 사용자 설정 ============ \n",
    "base_dir = r\"C:\\Users\\논문\"\n",
    "input_file = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "exclude_words = {\"미혼모\", \"하다\"}\n",
    "top_n = 15\n",
    "output_file_period = os.path.join(base_dir, \"TF_IDF123_시기별.xlsx\")\n",
    "\n",
    "\n",
    "# ============ 한글 폰트 설정 ============ \n",
    "font_path = \"C:/Windows/Fonts/malgun.ttf\"\n",
    "if os.path.exists(font_path):\n",
    "    fm.fontManager.addfont(font_path)\n",
    "    plt.rc(\"font\", family=\"Malgun Gothic\")\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# ============ 데이터 로드 ============ \n",
    "df = pd.read_excel(input_file, sheet_name=sheet_name)\n",
    "\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970–1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980–1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988–1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996–2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008–2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015–2023\"\n",
    "    else: return \"Unknown\"\n",
    "\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "\n",
    "# ============ 형태소 분석 및 전처리 ============ \n",
    "kiwi = Kiwi()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    words = []\n",
    "    for token in tokens:\n",
    "        tag = token.tag\n",
    "        word = token.form\n",
    "        if tag.startswith(\"NNG\"):\n",
    "            words.append(word)\n",
    "        elif tag.startswith(\"VV\"):\n",
    "            words.append(word + \"다\")\n",
    "    return [w for w in words if w not in exclude_words]\n",
    "\n",
    "df[\"tokens\"] = df[\"본문내용\"].astype(str).apply(lambda x: \" \".join(tokenize(x)))\n",
    "\n",
    "# ============ TF-IDF 계산 및 엑셀로 저장 ============ \n",
    "def compute_tfidf_to_excel(df, group_col, output_path):\n",
    "    grouped = df.groupby(group_col)[\"tokens\"].apply(lambda x: \" \".join(x))\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(grouped)\n",
    "    tfidf_df = pd.DataFrame(X.toarray(), index=grouped.index, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    top_words = set()\n",
    "    for _, row in tfidf_df.iterrows():\n",
    "        top_words.update(row.sort_values(ascending=False).head(top_n).index)\n",
    "    \n",
    "    tfidf_top = tfidf_df[list(top_words)]\n",
    "\n",
    "    # 엑셀 파일로 저장\n",
    "    if not tfidf_top.empty:\n",
    "        with pd.ExcelWriter(output_path) as writer:\n",
    "            tfidf_top.to_excel(writer, sheet_name=group_col)\n",
    "    else:\n",
    "        print(f\"⚠️ 엑셀로 저장되지 않았습니다: '{group_col}'에 유효한 TF-IDF 단어가 없습니다.\")\n",
    "\n",
    "# 실행\n",
    "compute_tfidf_to_excel(df, \"Period\", output_file_period)\n",
    "\n",
    "\n",
    "print(\"TF-IDF 결과가 엑셀 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a10d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from kiwipiepy import Kiwi\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# ============ 사용자 설정 ============ \n",
    "base_dir = r\"C:\\Users\\논문\"\n",
    "input_file = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "exclude_words = {\"미혼모\", \"하다\"}\n",
    "top_n = 10  # top 10 단어만\n",
    "\n",
    "# ============ 한글 폰트 설정 ============ \n",
    "font_path = \"C:/Windows/Fonts/malgun.ttf\"\n",
    "if os.path.exists(font_path):\n",
    "    fm.fontManager.addfont(font_path)\n",
    "    plt.rc(\"font\", family=\"Malgun Gothic\")\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# ============ 데이터 로드 ============ \n",
    "df = pd.read_excel(input_file, sheet_name=sheet_name)\n",
    "\n",
    "# ============ 형태소 분석 및 전처리 ============ \n",
    "kiwi = Kiwi()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    words = []\n",
    "    for token in tokens:\n",
    "        tag = token.tag\n",
    "        word = token.form\n",
    "        if tag.startswith(\"NNG\"):\n",
    "            words.append(word)\n",
    "        elif tag.startswith(\"VV\"):\n",
    "            words.append(word + \"다\")\n",
    "    return [w for w in words if w not in exclude_words]\n",
    "\n",
    "df[\"tokens\"] = df[\"본문내용\"].astype(str).apply(lambda x: \" \".join(tokenize(x)))\n",
    "\n",
    "# ============ TF-IDF 계산 ============ \n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"tokens\"])\n",
    "\n",
    "# TF-IDF 값을 데이터프레임으로 변환\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# 각 단어에 대해 TF-IDF 값의 총합을 계산\n",
    "tfidf_sums = tfidf_df.sum(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# 상위 10개 TF-IDF 총합이 높은 단어와 하위 10개 TF-IDF 총합이 낮은 단어 추출\n",
    "top_words = tfidf_sums.head(top_n)\n",
    "bottom_words = tfidf_sums.tail(top_n)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Top 10 TF-IDF 값이 높은 단어들:\")\n",
    "print(top_words)\n",
    "\n",
    "print(\"\\nTop 10 TF-IDF 값이 낮은 단어들:\")\n",
    "print(bottom_words)\n",
    "\n",
    "# ============ 시각화 (TF-IDF 총합) ============ \n",
    "def plot_tfidf_words(tfidf_series, title):\n",
    "    # 시각화\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    tfidf_series.plot(kind='barh', color='skyblue')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(\"total TF-IDF Weight\", fontsize=12)\n",
    "    plt.ylabel(\"words\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 높은 TF-IDF 값 단어 시각화\n",
    "plot_tfidf_words(top_words, \"Top 10 TF-IDF 값이 높은 단어들\")\n",
    "\n",
    "# 낮은 TF-IDF 값 단어 시각화\n",
    "plot_tfidf_words(bottom_words, \"Top 10 TF-IDF 값이 낮은 단어들\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c413c9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#시기별  sum TF-IDF는 여러 문서에서 등장해야 높아지므로 분산된 중요도를 잘 반영함. 그래서 아까 한 시기별 TF-IDF, 신문사별 TF-IDF 외에 이걸 다시\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# ========= 설정 =========\n",
    "base_dir = r\"C:\\Users\\논문\"\n",
    "file_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "EXCLUDE_WORDS = {\"미혼모\", \"하다\"}\n",
    "TOP_N = 20\n",
    "\n",
    "# ========= 데이터 불러오기 =========\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "# ========= 시기 분류 =========\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970–1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980–1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988–1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996–2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008–2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015–2023\"\n",
    "    else: return \"Unknown\"\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# ========= 형태소 분석기 & 토큰화 =========\n",
    "kiwi = Kiwi()\n",
    "def tokenize_text(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    return [\n",
    "        (token.form + \"다\" if token.tag.startswith(\"VV\") else token.form)\n",
    "        for token in tokens\n",
    "        if token.tag.startswith((\"NNG\", \"VV\")) and\n",
    "           ((token.form + \"다\") if token.tag.startswith(\"VV\") else token.form) not in EXCLUDE_WORDS\n",
    "    ]\n",
    "\n",
    "# ========= 시기별 문서 집계 =========\n",
    "period_docs = defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    period = row[\"Period\"]\n",
    "    tokens = tokenize_text(str(row[\"본문내용\"]))\n",
    "    period_docs[period].append(\" \".join(tokens))\n",
    "\n",
    "# ========= 전체 TF-IDF 계산 → Top 20 단어 선정 =========\n",
    "all_docs = []\n",
    "all_labels = []\n",
    "for period, docs in period_docs.items():\n",
    "    all_docs.extend(docs)\n",
    "    all_labels.extend([period] * len(docs))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_all = vectorizer.fit_transform(all_docs)\n",
    "df_all = pd.DataFrame(X_all.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "total_sum = df_all.sum(axis=0).sort_values(ascending=False)\n",
    "top_words = total_sum.head(TOP_N).index.tolist()\n",
    "\n",
    "# 시기별 sum TF-IDF (Top 20 단어만)\n",
    "period_sums = {}\n",
    "\n",
    "for period, docs in period_docs.items():\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # 누락된 단어는 0으로 추가\n",
    "    for word in top_words:\n",
    "        if word not in df_tfidf.columns:\n",
    "            df_tfidf[word] = 0.0\n",
    "\n",
    "    # top_words 순서대로 정렬\n",
    "    sum_tfidf = df_tfidf[top_words].sum(axis=0)\n",
    "    period_sums[period] = sum_tfidf\n",
    "\n",
    "# ========= 시기별 sum TF-IDF (Top 20 단어만) =========\n",
    "period_sums = {}\n",
    "for period, docs in period_docs.items():\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    sum_tfidf = df_tfidf[top_words].sum(axis=0)\n",
    "    period_sums[period] = sum_tfidf\n",
    "\n",
    "# ========= 결과 정리 =========\n",
    "result_df = pd.DataFrame(period_sums).T.fillna(0)\n",
    "output_path = os.path.join(base_dir, \"3\", \"시기별_top20_sumTFIDF.csv\")\n",
    "result_df.to_csv(output_path, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ 저장 완료: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b56650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#시각화\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 한글 폰트 설정 (Windows 기준)\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ========== 1. 꺾은선 그래프 ==========\n",
    "plt.figure(figsize=(12, 6))\n",
    "for column in result_df.columns:\n",
    "    plt.plot(result_df.index, result_df[column], marker='o', label=column)\n",
    "\n",
    "plt.title(\"시기별 Top 20 단어의 sum TF-IDF 변화\")\n",
    "plt.xlabel(\"시기\")\n",
    "plt.ylabel(\"sum TF-IDF\")\n",
    "plt.legend(loc=\"upper right\", bbox_to_anchor=(1.15, 1))\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========== 2. 히트맵 ==========\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(result_df, annot=True, fmt=\".2f\", cmap=\"YlGnBu\")\n",
    "plt.title(\"시기별 Top 20 단어의 sum TF-IDF 히트맵\")\n",
    "plt.xlabel(\"단어\")\n",
    "plt.ylabel(\"시기\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1b39a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# ========== 설정 ==========\n",
    "base_dir = r\"C:\\Users\\논문\"\n",
    "file_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "EXCLUDE_WORDS = {\"미혼모\", \"하다\"}\n",
    "TOP_N = 20\n",
    "\n",
    "# ========== 데이터 불러오기 ==========\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "# ========== 시기 분류 ==========\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970–1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980–1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988–1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996–2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008–2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015–2023\"\n",
    "    else: return \"Unknown\"\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# ========== 형태소 분석기 ==========\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    return [\n",
    "        (token.form + \"다\" if token.tag.startswith(\"VV\") else token.form)\n",
    "        for token in tokens\n",
    "        if token.tag.startswith((\"NNG\", \"VV\")) and\n",
    "           ((token.form + \"다\") if token.tag.startswith(\"VV\") else token.form) not in EXCLUDE_WORDS\n",
    "    ]\n",
    "\n",
    "# ========== 시기별 문서 토큰화 ==========\n",
    "period_docs = defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    period = row[\"Period\"]\n",
    "    tokens = tokenize_text(str(row[\"본문내용\"]))\n",
    "    period_docs[period].append(\" \".join(tokens))\n",
    "\n",
    "# ========== 전체 TF-IDF 계산 → Top N 단어 추출 ==========\n",
    "all_docs = []\n",
    "for docs in period_docs.values():\n",
    "    all_docs.extend(docs)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_all = vectorizer.fit_transform(all_docs)\n",
    "df_all = pd.DataFrame(X_all.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "top_words = df_all.sum().sort_values(ascending=False).head(TOP_N).index.tolist()\n",
    "\n",
    "# ========== 시기별 mean TF-IDF 계산 ==========\n",
    "period_mean = {}\n",
    "for period, docs in period_docs.items():\n",
    "    X = vectorizer.transform(docs)\n",
    "    df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # 누락 단어 보정\n",
    "    for word in top_words:\n",
    "        if word not in df_tfidf.columns:\n",
    "            df_tfidf[word] = 0.0\n",
    "\n",
    "    # 평균 TF-IDF\n",
    "    mean_tfidf = df_tfidf[top_words].mean(axis=0)\n",
    "    period_mean[period] = mean_tfidf\n",
    "\n",
    "# ========== 결과 정리 ==========\n",
    "mean_tfidf_df = pd.DataFrame(period_mean).T[top_words]  # Period x Top 20 단어\n",
    "output_path = os.path.join(base_dir, \"3\", \"시기별_top20_meanTFIDF.csv\")\n",
    "mean_tfidf_df.to_csv(output_path, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ 시기별 평균 TF-IDF 결과 저장 완료: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25837a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 한글 폰트 설정 (Windows 기준)\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ========== 1. 꺾은선 그래프 ==========\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for word in mean_tfidf_df.columns:\n",
    "    plt.plot(mean_tfidf_df.index, mean_tfidf_df[word], marker='o', label=word)\n",
    "\n",
    "plt.title(\"시기별 Top 20 단어의 평균 TF-IDF 변화\")\n",
    "plt.xlabel(\"시기\")\n",
    "plt.ylabel(\"Mean TF-IDF\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.15, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========== 2. 히트맵 ==========\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(mean_tfidf_df, annot=True, fmt=\".4f\", cmap=\"YlGnBu\")\n",
    "\n",
    "plt.title(\"시기별 Top 20 단어의 평균 TF-IDF 히트맵\")\n",
    "plt.xlabel(\"단어\")\n",
    "plt.ylabel(\"시기\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a4a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#z-score로 중요도 분석 (Z-score 값 기반 단어의 상대 중요도), mean tf-idf의 z-score\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# ========== 설정 ==========\n",
    "base_dir = r\"C:\\Users\\논문\"\n",
    "file_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "EXCLUDE_WORDS = {\"미혼모\", \"하다\"}\n",
    "TOP_N = 20\n",
    "\n",
    "# ========== 데이터 불러오기 ==========\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "# ========== 시기 분류 ==========\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970–1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980–1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988–1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996–2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008–2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015–2023\"\n",
    "    else: return \"Unknown\"\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# ========== 형태소 분석기 ==========\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    return [\n",
    "        (token.form + \"다\" if token.tag.startswith(\"VV\") else token.form)\n",
    "        for token in tokens\n",
    "        if token.tag.startswith((\"NNG\", \"VV\")) and\n",
    "           ((token.form + \"다\") if token.tag.startswith(\"VV\") else token.form) not in EXCLUDE_WORDS\n",
    "    ]\n",
    "\n",
    "# ========== 시기별 문서 토큰화 ==========\n",
    "period_docs = defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    period = row[\"Period\"]\n",
    "    tokens = tokenize_text(str(row[\"본문내용\"]))\n",
    "    period_docs[period].append(\" \".join(tokens))\n",
    "\n",
    "# ========== 전체 TF-IDF 계산 → Top N 단어 추출 ==========\n",
    "all_docs = []\n",
    "for docs in period_docs.values():\n",
    "    all_docs.extend(docs)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_all = vectorizer.fit_transform(all_docs)\n",
    "df_all = pd.DataFrame(X_all.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "top_words = df_all.sum().sort_values(ascending=False).head(TOP_N).index.tolist()\n",
    "\n",
    "# ========== 시기별 mean TF-IDF 계산 ==========\n",
    "period_mean = {}\n",
    "for period, docs in period_docs.items():\n",
    "    X = vectorizer.transform(docs)\n",
    "    df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # 누락 단어 보정\n",
    "    for word in top_words:\n",
    "        if word not in df_tfidf.columns:\n",
    "            df_tfidf[word] = 0.0\n",
    "\n",
    "    # 평균 TF-IDF\n",
    "    mean_tfidf = df_tfidf[top_words].mean(axis=0)\n",
    "    period_mean[period] = mean_tfidf\n",
    "\n",
    "# ========== 결과 정리 ==========\n",
    "mean_tfidf_df = pd.DataFrame(period_mean).T[top_words]  # Period x Top 20 단어\n",
    "\n",
    "# ========== Z-score 계산 ==========\n",
    "mean_val = mean_tfidf_df.values.mean()\n",
    "std_val = mean_tfidf_df.values.std()\n",
    "z_score_df = (mean_tfidf_df - mean_val) / std_val\n",
    "\n",
    "# ========== 저장 ==========\n",
    "output_dir = os.path.join(base_dir, \"3\")\n",
    "z_out = os.path.join(output_dir, \"시기별_top20_zscore.csv\")\n",
    "\n",
    "mean_tfidf_df.to_csv(mean_out, encoding=\"utf-8-sig\")\n",
    "z_score_df.to_csv(z_out, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "print(f\"✅ Z-score 저장: {z_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1983d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum tf-idf 에서 z-score까지 구하기\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# ========== 설정 ==========\n",
    "base_dir = r\"C:\\Users\\논문\"\n",
    "file_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "EXCLUDE_WORDS = {\"미혼모\", \"하다\"}\n",
    "TOP_N = 20\n",
    "\n",
    "# ========== 데이터 불러오기 ==========\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "# ========== 시기 분류 ==========\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970–1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980–1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988–1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996–2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008–2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015–2023\"\n",
    "    else: return \"Unknown\"\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# ========== 형태소 분석기 ==========\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    return [\n",
    "        (token.form + \"다\" if token.tag.startswith(\"VV\") else token.form)\n",
    "        for token in tokens\n",
    "        if token.tag.startswith((\"NNG\", \"VV\")) and\n",
    "           ((token.form + \"다\") if token.tag.startswith(\"VV\") else token.form) not in EXCLUDE_WORDS\n",
    "    ]\n",
    "\n",
    "# ========== 시기별 문서 토큰화 ==========\n",
    "period_docs = defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    period = row[\"Period\"]\n",
    "    tokens = tokenize_text(str(row[\"본문내용\"]))\n",
    "    period_docs[period].append(\" \".join(tokens))\n",
    "\n",
    "# ========== 전체 TF-IDF 계산 → Top N 단어 추출 ==========\n",
    "all_docs = []\n",
    "for docs in period_docs.values():\n",
    "    all_docs.extend(docs)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_all = vectorizer.fit_transform(all_docs)\n",
    "df_all = pd.DataFrame(X_all.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "top_words = df_all.sum().sort_values(ascending=False).head(TOP_N).index.tolist()\n",
    "\n",
    "# ========== 시기별 sum TF-IDF 계산 ==========\n",
    "period_sum = {}\n",
    "for period, docs in period_docs.items():\n",
    "    X = vectorizer.transform(docs)\n",
    "    df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # 누락된 단어는 0으로 채우기\n",
    "    for word in top_words:\n",
    "        if word not in df_tfidf.columns:\n",
    "            df_tfidf[word] = 0.0\n",
    "\n",
    "    # sum TF-IDF\n",
    "    sum_tfidf = df_tfidf[top_words].sum(axis=0)\n",
    "    period_sum[period] = sum_tfidf\n",
    "\n",
    "sum_tfidf_df = pd.DataFrame(period_sum).T[top_words]\n",
    "\n",
    "# ========== Z-score 정규화 ==========\n",
    "mean_val = sum_tfidf_df.values.mean()\n",
    "std_val = sum_tfidf_df.values.std()\n",
    "z_score_sum_df = (sum_tfidf_df - mean_val) / std_val\n",
    "\n",
    "# ========== 저장 ==========\n",
    "output_dir = os.path.join(base_dir, \"3\")\n",
    "sum_out = os.path.join(output_dir, \"시기별_top20_sumTFIDF.csv\")\n",
    "z_out = os.path.join(output_dir, \"시기별_top20_sum_zscore.csv\")\n",
    "\n",
    "# 열려있는 파일 방지 → 파일명 바꿔 저장 시도\n",
    "try:\n",
    "    sum_tfidf_df.to_csv(sum_out, encoding=\"utf-8-sig\")\n",
    "    z_score_sum_df.to_csv(z_out, encoding=\"utf-8-sig\")\n",
    "    print(f\"✅ sum TF-IDF 저장 완료: {sum_out}\")\n",
    "    print(f\"✅ Z-score 저장 완료: {z_out}\")\n",
    "except PermissionError:\n",
    "    alt_sum_out = os.path.join(output_dir, \"시기별_top20_sumTFIDF_v2.csv\")\n",
    "    alt_z_out = os.path.join(output_dir, \"시기별_top20_sum_zscore_v2.csv\")\n",
    "    sum_tfidf_df.to_csv(alt_sum_out, encoding=\"utf-8-sig\")\n",
    "    z_score_sum_df.to_csv(alt_z_out, encoding=\"utf-8-sig\")\n",
    "    print(f\"⚠️ 기존 파일 열려 있어 다른 이름으로 저장함:\")\n",
    "    print(f\"✔️ sum TF-IDF → {alt_sum_out}\")\n",
    "    print(f\"✔️ Z-score → {alt_z_out}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f79ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# ========== 설정 ==========\n",
    "base_dir = r\"C:\\Users\\논문\"\n",
    "file_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "EXCLUDE_WORDS = {\"미혼모\", \"하다\"}\n",
    "TOP_N = 20\n",
    "\n",
    "# ========== 데이터 불러오기 ==========\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "# ========== 시기 분류 ==========\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970–1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980–1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988–1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996–2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008–2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015–2023\"\n",
    "    else: return \"Unknown\"\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# ========== 형태소 분석기 ==========\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    return [\n",
    "        (token.form + \"다\" if token.tag.startswith(\"VV\") else token.form)\n",
    "        for token in tokens\n",
    "        if token.tag.startswith((\"NNG\", \"VV\")) and\n",
    "           ((token.form + \"다\") if token.tag.startswith(\"VV\") else token.form) not in EXCLUDE_WORDS\n",
    "    ]\n",
    "\n",
    "# ========== 시기별 문서 토큰화 ==========\n",
    "period_docs = defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    period = row[\"Period\"]\n",
    "    tokens = tokenize_text(str(row[\"본문내용\"]))\n",
    "    period_docs[period].append(\" \".join(tokens))\n",
    "\n",
    "# ========== 전체 TF-IDF 계산 → Top N 단어 추출 ==========\n",
    "all_docs = []\n",
    "for docs in period_docs.values():\n",
    "    all_docs.extend(docs)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_all = vectorizer.fit_transform(all_docs)\n",
    "df_all = pd.DataFrame(X_all.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "top_words = df_all.sum().sort_values(ascending=False).head(TOP_N).index.tolist()\n",
    "\n",
    "# ========== 시기별 mean TF-IDF 계산 ==========\n",
    "period_mean = {}\n",
    "for period, docs in period_docs.items():\n",
    "    X = vectorizer.transform(docs)\n",
    "    df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # 누락 단어 보정\n",
    "    for word in top_words:\n",
    "        if word not in df_tfidf.columns:\n",
    "            df_tfidf[word] = 0.0\n",
    "\n",
    "    # 평균 TF-IDF\n",
    "    mean_tfidf = df_tfidf[top_words].mean(axis=0)\n",
    "    period_mean[period] = mean_tfidf\n",
    "\n",
    "# ========== 결과 정리 ==========\n",
    "mean_tfidf_df = pd.DataFrame(period_mean).T[top_words]  # Period x Top 20 단어\n",
    "\n",
    "# ========== Z-score 계산 ==========\n",
    "mean_val = mean_tfidf_df.values.mean()\n",
    "std_val = mean_tfidf_df.values.std()\n",
    "z_score_df = (mean_tfidf_df - mean_val) / std_val\n",
    "\n",
    "\n",
    "# ====== 폰트 설정 (Windows 기준) ======\n",
    "plt.rcParams[\"font.family\"] = \"Malgun Gothic\"  # Windows\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# ========== 2. 히트맵 ==========\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(z_score_df, annot=True, fmt=\".4f\", cmap=\"YlGnBu\")\n",
    "\n",
    "plt.title(\"시기별 Top 20 단어의 평균 TF-IDF z-score 히트맵\")\n",
    "plt.xlabel(\"단어\")\n",
    "plt.ylabel(\"시기\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56eb5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf 전체 단어 수\n",
    "\n",
    "#word frequency\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from kiwipiepy import Kiwi\n",
    "from collections import Counter\n",
    "\n",
    "# 경로 설정\n",
    "base_dir = r\"C:\\Users\\논문\"\n",
    "input_path = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text\"\n",
    "\n",
    "# 데이터 불러오기\n",
    "df = pd.read_excel(input_path, sheet_name=sheet_name)\n",
    "\n",
    "# 시기 구분\n",
    "def get_period(year):\n",
    "    if 1970 <= year <= 1979: return \"1. 1970–1979\"\n",
    "    elif 1980 <= year <= 1987: return \"2. 1980–1987\"\n",
    "    elif 1988 <= year <= 1995: return \"3. 1988–1995\"\n",
    "    elif 1996 <= year <= 2007: return \"4. 1996–2007\"\n",
    "    elif 2008 <= year <= 2014: return \"5. 2008–2014\"\n",
    "    elif 2015 <= year <= 2023: return \"6. 2015–2023\"\n",
    "    else: return \"Unknown\"\n",
    "\n",
    "df[\"Period\"] = df[\"Year\"].apply(get_period)\n",
    "\n",
    "# 형태소 분석기\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# 분석할 품사\n",
    "target_pos = [\"NNG\", \"VV\", \"VA\", \"MAG\"]\n",
    "\n",
    "def analyze_tokens(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    counters = {\n",
    "        \"NNG\": Counter(),\n",
    "        \"VV\": Counter(),\n",
    "        \"VA\": Counter(),\n",
    "        \"MAG\": Counter(),\n",
    "        \"COMBINED\": Counter()\n",
    "    }\n",
    "    for token in tokens:\n",
    "        word = token.form\n",
    "        tag = token.tag\n",
    "        if tag == \"VV\":\n",
    "            word += \"다\"\n",
    "        if tag == \"VA\":\n",
    "            word += \"다\"\n",
    "        if tag in target_pos:\n",
    "            counters[tag][word] += 1\n",
    "            counters[\"COMBINED\"][word] += 1\n",
    "    return counters\n",
    "\n",
    "# === 전체 텍스트 분석 ===\n",
    "full_text = \" \".join(df[\"본문내용\"].astype(str).tolist())\n",
    "total_counts = analyze_tokens(full_text)\n",
    "\n",
    "# 결과 저장\n",
    "top_combined = total_counts[\"COMBINED\"].most_common(20)\n",
    "top_nouns = total_counts[\"NNG\"].most_common(10)\n",
    "top_verbs = total_counts[\"VV\"].most_common(10)\n",
    "\n",
    "# === 신문사별 / 시기별 ===\n",
    "def grouped_word_counts(df, by=\"신문사명\"):\n",
    "    result = {}\n",
    "    for key, group in df.groupby(by):\n",
    "        text = \" \".join(group[\"본문내용\"].astype(str).tolist())\n",
    "        counts = analyze_tokens(text)\n",
    "        result[key] = {\n",
    "            \"COMBINED\": counts[\"COMBINED\"].most_common(10),\n",
    "            \"NNG\": counts[\"NNG\"].most_common(5),\n",
    "            \"VV\": counts[\"VV\"].most_common(5)\n",
    "        }\n",
    "    return result\n",
    "\n",
    "newspaper_results = grouped_word_counts(df, by=\"신문사명\")\n",
    "period_results = grouped_word_counts(df, by=\"Period\")\n",
    "\n",
    "# 품사별 전체 단어 수 출력\n",
    "total_noun_count = sum(total_counts[\"NNG\"].values())\n",
    "total_verb_count = sum(total_counts[\"VV\"].values())\n",
    "total_adj_count = sum(total_counts[\"VA\"].values())\n",
    "total_adv_count = sum(total_counts[\"MAG\"].values())\n",
    "total_combined_count = sum(total_counts[\"COMBINED\"].values())\n",
    "\n",
    "print(\"\\n✅ 전체 단어 수 (빈도 기준):\")\n",
    "print(f\"  - 전체 합계: {total_combined_count}\")\n",
    "print(f\"  - 명사(NNG): {total_noun_count}\")\n",
    "print(f\"  - 동사(VV): {total_verb_count}\")\n",
    "print(f\"  - 형용사(VA): {total_adj_count}\")\n",
    "print(f\"  - 부사(MAG): {total_adv_count}\")\n",
    "\n",
    "\n",
    "# 출력 예시\n",
    "print(\"✅ 전체 텍스트 Top 20:\")\n",
    "print(top_combined)\n",
    "\n",
    "print(\"\\n✅ 전체 텍스트 Top 10 명사:\")\n",
    "print(top_nouns)\n",
    "\n",
    "print(\"\\n✅ 전체 텍스트 Top 10 동사:\")\n",
    "print(top_verbs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf1201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# ===== 사용자 경로 설정 =====\n",
    "base_dir = r\"C:\\Users\\논문\"\n",
    "input_file = os.path.join(base_dir, \"3\", \"coding.xlsx\")\n",
    "sheet_name = \"full text_기사\"\n",
    "exclude_words = {\"미혼모\", \"하다\"}\n",
    "\n",
    "# ===== 데이터 불러오기 =====\n",
    "df = pd.read_excel(input_file, sheet_name=sheet_name)\n",
    "\n",
    "# ===== 형태소 분석 및 전처리 =====\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    words = []\n",
    "    for token in tokens:\n",
    "        if token.tag.startswith(\"NNG\"):\n",
    "            words.append(token.form)\n",
    "        elif token.tag.startswith(\"VV\"):\n",
    "            words.append(token.form + \"다\")\n",
    "    return [w for w in words if w not in exclude_words]\n",
    "\n",
    "df[\"tokens\"] = df[\"본문내용\"].astype(str).apply(lambda x: \" \".join(tokenize(x)))\n",
    "\n",
    "# ===== TF-IDF 계산 =====\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"tokens\"])\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out(), index=df.index)\n",
    "\n",
    "# ===== TF-IDF 합산 (단어별) =====\n",
    "word_sum_tfidf = tfidf_df.sum(axis=0).to_frame(name=\"sumTF-IDF\")\n",
    "\n",
    "# ===== 기술통계 요약 =====\n",
    "word_sum_stats = word_sum_tfidf[\"sumTF-IDF\"].describe(percentiles=[.25, .5, .75]).round(3)\n",
    "stats_df = pd.DataFrame(word_sum_stats).T\n",
    "stats_df.index = [\"sumTF-IDF\"]\n",
    "stats_df = stats_df[[\"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"]]\n",
    "\n",
    "# ===== 결과 출력 =====\n",
    "print(\"📊 단어별 TF-IDF 총합 통계:\")\n",
    "print(stats_df)\n",
    "\n",
    "# ===== 선택: LaTeX 테이블로 저장 =====\n",
    "latex_output_path = os.path.join(base_dir, \"TFIDF_sum_stats_by_word.tex\")\n",
    "with open(latex_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(stats_df.to_latex(index=True, caption=\"단어별 TF-IDF 총합 통계\", label=\"tab:word_tfidf_stats\"))\n",
    "\n",
    "print(f\"\\n📄 LaTeX 테이블 저장 완료: {latex_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518ab82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 단어별 sumTF-IDF 계산 =====\n",
    "word_sum_tfidf = tfidf_df.sum(axis=0).to_frame(name=\"sumTF-IDF\")\n",
    "\n",
    "# ===== 숫자 포맷을 위한 복사본 생성 =====\n",
    "word_sum_tfidf_fmt = word_sum_tfidf.copy()\n",
    "word_sum_tfidf_fmt[\"sumTF-IDF\"] = word_sum_tfidf_fmt[\"sumTF-IDF\"].apply(lambda x: f\"{x:,.0f}\")\n",
    "\n",
    "# ===== 상위 10개 단어 =====\n",
    "top_10 = word_sum_tfidf_fmt.sort_values(by=\"sumTF-IDF\", ascending=False).head(10)\n",
    "top_10.index.name = \"Top 10 (High TF-IDF)\"\n",
    "\n",
    "# ===== 하위 10개 단어 =====\n",
    "bottom_10 = word_sum_tfidf_fmt.sort_values(by=\"sumTF-IDF\", ascending=True).head(10)\n",
    "bottom_10.index.name = \"Top 10 (Low TF-IDF)\"\n",
    "\n",
    "# ===== 출력 =====\n",
    "print(\"🔝 TF-IDF 상위 10개 단어:\")\n",
    "print(top_10)\n",
    "print(\"\\n🔻 TF-IDF 하위 10개 단어:\")\n",
    "print(bottom_10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff403c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
